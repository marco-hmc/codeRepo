# 基础知识

## [Linux的启动过程](https://www.runoob.com/linux/linux-system-boot.html)

* **内核引导**：当计算机打开电源后，首先是**BIOS开机自检**，按照BIOS中设置的启动设备（通常是硬盘）来启动。操作系统接管硬件以后，首先读入 /boot 目录下的内核文件

* **运行init**：init 进程是系统所有进程的起点，你可以把它比拟成系统所有进程的老祖宗，没有这个进程，系统中任何进程都不会启动。init 程序首先是需要读取配置文件 /etc/inittab

  * 运行级别
    * 许多程序需要开机启动。它们在Windows叫做"服务"（service），在Linux就叫做"守护进程"（daemon）。init进程的一大任务，就是去运行这些开机启动的程序。但是，**不同的场合需要启动不同的程序**，比如用作服务器时，需要启动Apache，用作桌面就不需要
    * Linux允许为**不同的场合，分配不同的开机启动程序，这就叫做"运行级别"（runlevel）**。也就是说，启动时**根据"运行级别"，确定要运行哪些程序**
    * Linux系统有7个运行级别(runlevel)：
      - 运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动
      - 运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆
      - 运行级别2：多用户状态(没有NFS)
      - 运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式
      - 运行级别4：系统未使用，保留
      - 运行级别5：X11控制台，登陆后进入图形GUI模式
      - 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动

* 系统初始化：在init的配置文件中有这么一行： si::sysinit:/etc/rc.d/rc.sysinit　它调用执行了/etc/rc.d/rc.sysinit，而rc.sysinit是一个bash shell的脚本，它主要是完成一些系统初始化的工作，rc.sysinit是每一个运行级别都要首先运行的重要脚本。它主要完成的工作有：**激活交换分区，检查磁盘，加载硬件模块以及其它一些需要优先执行任务**

* 建立终端：rc执行完毕后，返回init。这时基本系统环境已经设置好了，各种守护进程也已经启动了。init接下来会打开6个终端，以便用户登录系统。在inittab中的以下6行就是定义了6个终端

  ```shell
  1:2345:respawn:/sbin/mingetty tty1
  2:2345:respawn:/sbin/mingetty tty2
  3:2345:respawn:/sbin/mingetty tty3
  4:2345:respawn:/sbin/mingetty tty4
  5:2345:respawn:/sbin/mingetty tty5
  6:2345:respawn:/sbin/mingetty tty6
  ```

  从上面可以看出在2、3、4、5的运行级别中都将以respawn方式运行mingetty程序，mingetty程序能打开终端、设置模式。同时它会显示一个文本登录界面，这个界面就是我们经常看到的登录界面，在这个登录界面中会提示用户输入用户名，而用户输入的用户将作为参数传给login程序来验证用户的身份。

* 用户登录系统：一般来说，用户的登录方式有三种：

  - 命令行登录
  - ssh登录
  - 图形界面登录

* 对于运行级别为5的图形方式用户来说，他们的登录是通过一个图形化的登录界面。登录成功后可以直接进入 KDE、Gnome 等窗口管理器。而本文主要讲的还是文本方式登录的情况：当我们看到mingetty的登录界面时，我们就可以输入用户名和密码来登录系统了。Linux 的账号验证程序是 login，login 会接收 mingetty 传来的用户名作为用户名参数。然后 login 会对用户名进行分析：如果用户名不是 root，且存在 /etc/nologin 文件，login 将输出 nologin 文件的内容，然后退出。这通常用来系统维护时防止非root用户登录。**只有/etc/securetty中登记了的终端才允许 root 用户登录**，如果不存在这个文件，则 root 用户可以在任何终端上登录。/etc/usertty文件用于对用户作出附加访问限制，如果不存在这个文件，则没有其他限制

* 图形模式与文字模式的切换方式

  * Linux预设提供了**六个命令窗口终端机让我们来登录**
  * 默认我们登录的就是第一个窗口，也就是tty1，这个六个窗口分别为tty1,tty2 … tty6，你可以按下Ctrl + Alt + F1 ~ F6 来切换它们
  * 如果你安装了图形界面，默认情况下是进入图形界面的，此时你就可以按Ctrl + Alt + F1 ~ F6来进入其中一个命令窗口界面
  * 当你进入命令窗口界面后再返回图形界面只要按下Ctrl + Alt + F7 就回来了
  * 如果你用的vmware 虚拟机，命令窗口切换的快捷键为 Alt + Space + F1~F6. 如果你在图形界面下请按Alt + Shift + Ctrl + F1~F6 切换至命令窗口

# 进程与线程

## fork，vfork，clone

* fork，vfork，clone**都是linux的系统调用**，这三个函数分别调用了**sys_fork、sys_vfork、sys_clone**，最终都调用了do_fork函数，**差别在于参数的传递和一些基本的准备工作不同，主要用来linux创建新的子进程或线程**（vfork创造出来的是线程）

  ![fork_vfork_clone](imgs/os/fork_vfork_clone.png)

* fork

  * 函数调用成功：返回两个值； 父进程：返回子进程的PID；子进程：返回0；失败：返回-1

  * 写时复制技术（Copy-On-Write）：其基础的观念是，如果有多个调用者(callers)同时要求相同资源，他们会共同取得相同的指标指向相同的资源，直到某个调用者(caller)尝试修改资源时，系统才会真正复制一个副本(private copy)给该呼叫者，以避免被修改的资源被直接察觉到，这过程对其他的呼叫只都是通透的(transparently)。**此作法主要的优点是如果呼叫者并没有修改该资源，就不会有副本(private copy)被建立**

  * 现在Linux系统调用fork利用COW技术，**父进程和子进程共享页帧而不是复制页帧**。然而，只要页帧被共享，它们就不能被修改，即页帧被保护。**无论父进程还是子进程何时试图写一个共享的页帧，就产生一个异常，这时内核就把这个页复制到一个新的页帧中并标记为可写**。原来的页帧仍然是写保护的：当其他进程试图写入时，内核检查写进程是否是这个页帧的唯一属主，如果是，就把这个页帧标记为对这个进程是可写的

  * fork前后内存关系

    * fork()创建了一个心的进程(child)信进程几乎是调用进程(父进程的翻版),理解fork()的关键是，在完成对其调用之后，会产生2个进程，且每个进程都会从fork()的返回处开始执行

    * 这俩个进程将**执行相同的程序段**，但是**拥有各自不同的堆段，栈段，数据段**，每个子程序都可修改各自的数据段，堆段，和栈段

    * 调用fork()之后先执行哪个进程的是由Linux下专有文件/proc/sys/kernel/sched_child_runs_first的值来确定的(值为0父进程先执行，非0子进程先执行)

    * fork后**子进程只复制父进程的页表，父子进程的代码段是相同的，所以代码段是没必要复制的，因此内核将代码段标记为只读，这样父子进程就可以安全的共享此代码段了**。fork之后在进程创建代码段时，新子进程的进程级页表项都指向和父进程相同的物理页帧

      <img src="imgs/os/fork_mem.png" alt="fork_mem" style="zoom: 80%;" />

    * **而对于父进程的数据段，堆段，栈段中的各页，由于父子进程要相互独立，所以我们采用写实复制的技术，来最大化的提高内存以及内核的利用率。刚开始，内核做了一些设置，令这些段的页表项指向父进程相同的物理内存页。调用fork之后，内核会捕获所有父进程或子进程针对这些页面的修改企图(说明此时还没修改)并为将要修改的页面创建拷贝。系统将新的页面拷贝分配给被内核捕获的进程，还会对子进程的相应页表项做适当的调整，现在父子进程就可以分别修改各自的上述段，不再互相影响了**

    * COW前

      <img src="imgs/os/fork_cow1.png" alt="fork_cow1" style="zoom:80%;" />

    * COW后

      <img src="imgs/os/fork_cow2.png" alt="fork_cow2" style="zoom:80%;" />

* vfork

  * 是一个过时的应用，**vfork也是创建一个子进程，但是子进程共享父进程的空间**。**在vfork创建子进程之后，父进程阻塞，直到子进程执行了exec()或者exit()**
  * **vfork最初是因为fork没有实现COW机制，而很多情况下fork之后会紧接着exec，而exec的执行相当于之前fork复制的空间全部变成了无用功，所以设计了vfork**。而现在fork使用了COW机制，唯一的代价仅仅是复制父进程页表的代价，所以vfork不应该出现在新的代码之中
  * **由vfork创建的子进程要先于父进程执行，子进程执行时，父进程处于挂起状态，子进程执行完，唤醒父进程**。除非子进程exit或者execve才会唤起父进程
  * vfork()用法与fork()相似，但是也有区别，具体区别归结为以下3点
    * **fork() 子进程拷贝父进程的数据段，代码段，vfork() 子进程与父进程共享数据段**
    * **fork() 父子进程的执行次序不确定，vfork()，保证子进程先运行**
    * **vfork()保证子进程先运行，在它调用exec或_exit之后父进程才可能被调度运行**。如果**在调用这两个函数之前子进程依赖于父进程的进一步动作，则会导致死锁**

* clone

  * 是Linux**为创建线程设计**的（虽然也可以用clone创建进程）。所以可以说**clone是fork的升级版本，不仅可以创建进程或者线程，还可以指定创建新的命名空间（namespace）、有选择的继承父进程的内存、甚至可以将创建出来的进程变成父进程的兄弟进程等等**
  * clone函数功能强大，带了众多参数，它提供了一个非常灵活自由的常见进程的方法。因此由他创建的进程要比前面2种方法要复杂。**clone可以让你有选择性的继承父进程的资源，你可以选择像vfork一样和父进程共享一个虚存空间，从而使创造的是线程，你也可以不和父进程共享，你甚至可以选择创造出来的进程和父进程不再是父子关系，而是兄弟关系**

  ```c++
  int clone(int (*fn)(void *), void *child_stack, int flags, void *arg);
  
  // fn为函数指针，此指针指向一个函数体，即想要创建进程的静态程序（我们知道进程的4要素，这个就是指向程序的指针，就是所谓的“剧本"）；
  // child_stack为给子进程分配系统堆栈的指针（在linux下系统堆栈空间是2页面，就是8K的内存，其中在这块内存中，低地址上放入了值，这个值就是进程控制块task_struct的值）；
  // arg就是传给子进程的参数一般为（0）；
  // flags为要复制资源的标志，描述你需要从父进程继承那些资源
  ```

  以下是flags可取的值

  <img src="imgs/os/clone.png" alt="clone" style="zoom:80%;" />

* clone和fork的区别：

  * clone和fork的调用方式很不相同，clone调用需要传入一个函数，该函数在子进程中执行

* clone和fork最大不同在于**clone不再复制父进程的栈空间，而是自己创建一个新的**。 （void *child_stack）也就是第二个参数，需要分配栈指针的空间大小，所以**它不再是继承或者复制，而是全新的创造**

## 孤儿进程和僵尸进程

* **孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作**
  * **孤儿进程并不会有什么危害**

* **僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵尸进程**、

  * 任何一个子进程(init除外)在exit()之后，并非马上就消失掉，而是留下一个称为僵尸进程(Zombie)的数据结构，等待父进程处理
  * 危害：如果进程不调用wait / waitpid的话，那么保留的那段信息就不会释放，其**进程号就会一直被占用**，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程
  * 解决办法：子进程退出时向父进程发送SIGCHILD信号，父进程处理SIGCHILD信号，在处理函数中调用wait或者waitpid

* 僵尸进程危害场景：

  　　* 例如有个进程，它**定期的产生一个子进程**，这个子进程需要做的事情很少，做完它该做的事情之后就退出了，因此这个子进程的生命周期很短，但是，**父进程只管生成新的子进程，至于子进程 退出之后的事情，则一概不闻不问，这样，系统运行上一段时间之后，系统中就会存在很多的僵死进程**，倘若用ps命令查看的话，就会看到很多状态为Z的进程
    　　* 严格地来说，僵死进程并不是问题的根源，**罪魁祸首是产生出大量僵死进程的那个父进程**。因此，当我们寻求如何消灭系统中大量的僵死进程时，答案就是**把产生大量僵尸进程的那个元凶枪毙掉**（也就是通过kill发送SIGTERM或者SIGKILL信号啦）。枪毙了元凶进程之后，它产生的僵死进程就变成了孤儿进程，这些孤儿进程会被init进程接管，init进程会wait()这些孤儿进程，释放它们占用的系统进程表中的资源，这样，这些已经僵死的孤儿进程 就能瞑目而去了


# [守护进程](https://blog.csdn.net/mijichui2153/article/details/81394387)

* 守护进程概念

  * 守护进程是**后台运行的**、**不与任何终端关联的(无法通过终端输入输出)**，用于**周期性地执行某种任务或等待处理特定的事件**

* 守护进程实现思路

  * **现一个守护进程，其实就是将普通进程按照上述特性改造为守护进程的过程。**需要注意的一点是，不同版本的 Unix 系统其实现机制不同，BSD 和 Linux 下的实现细节就不同

* 进程组和会话的概念

  * 进程组是**一组相关进程的集合**
  * 会话是**一组相关进程组的集合**

* 一个进程会有如下ID

  * PID：进程的唯一标识。对于多线程的进程而言，所有线程调用`getpid`函数会返回相同的值

  * PGID：进程组ID。每个进程都会有进程组ID，表示该进程所属的进程组。**默认情况下新创建的进程会继承父进程的进程组ID**

  * SID：会话ID。每个进程也都有会话ID。默认情况下，**新创建的进程会继承父进程的会话ID**

  * 可以调用以下函数获取/设置进程组ID跟会话ID

    ```c
    #include<unistd.h>
    #include<sys/types.h>
    pid_t getpgrp(void);
    pid_t getsid(pid_t pid); 
    int setpgid(pid_t pid, pid_t pgid); // 找到进程ID为pid的进程，将其进程组ID修改为pgid，如果pid的值为0，则表示要修改调用进程的进程组ID。该接口一般用来创建一个新的进程组
    
    pid_t setsid(void);  
    /*
    如果这个函数的调用进程不是进程组组长，那么调用该函数会发生以下事情：
    1）创建一个新会话，会话ID等于进程ID，调用进程成为会话的首进程。
    2）创建一个进程组，进程组ID等于进程ID，调用进程成为进程组的组长。
    3）该进程没有控制终端，如果调用setsid前，该进程有控制终端，这种联系就会断掉。
    调用setsid函数的进程不能是进程组的组长，否则调用会失败，返回-1，并置errno为EPERM。
    这个限制是比较合理的。如果允许进程组组长迁移到新的会话，而进程组的其他成员仍然在老的会话中，那么，就会出现同一个进程组的进程分属不同的会话之中的情况，这就破坏了进程组和会话的严格的层次关系了。
    */
    ```

* 创建后台进程步骤

  * **fork()创建子进程，父进程exit()退出**

    * 这是创建守护进程的第一步。**由于守护进程是脱离控制终端的，完成这一步后就会在Shell终端里造成程序已经运行完毕的假象**。之后的所有工作都在子进程中完成，而用户在Shell终端里则可以执行其他命令，从而在形式上做到了与控制终端的脱离，在后台工作
    * 由于**父进程先于子进程退出**，**子进程就变为孤儿进程**，并由` init `进程作为其父进程收养

  * **在子进程调用setsid()创建新会话**

    * 在调用了 fork() 函数后，子进程全盘拷贝了父进程的会话期、进程组、控制终端等，**虽然父进程退出了，但会话期、进程组、控制终端等并没有改变。这还不是真正意义上的独立开来，而 setsid() 函数能够使进程完全独立出来**
    * setsid()创建一个新会话，**调用进程担任新会话的首进程**，其作用有
      * 使当前进程脱离原会话的控制
      * 使当前进程脱离原进程组的控制
      * 使当前进程脱离原控制终端的控制
    * 这样，当前进程才能实现真正意义上完全独立出来，摆脱其他进程的控制

  * **再次 fork() 一个子进程，父进程exit()退出**

    * 现在，进程已经成为无终端的会话组长，但它可以重新申请打开一个控制终端，可以通过 fork() 一个子进程，该子进程不是会话首进程，该进程将不能重新打开控制终端。退出父进程
    * 也就是说**通过再次创建子进程结束当前进程，使进程不再是会话首进程来禁止进程重新打开控制终端**

  * **在子进程中调用chdir()让根目录“/”成为子进程的工作目录**

    * 这一步也是**必要的步骤**。**使用fork创建的子进程继承了父进程的当前工作目录**。由于在进程运行中，当前目录所在的文件系统（如“/mnt/usb”）是不能卸载的，这对以后的使用会造成诸多的麻烦（比如系统由于某种原因要进入单用户模式）
    * 因此，通常的做法是让"/"作为守护进程的当前工作目录，这样就可以避免上述的问题，当然，如有特殊需要，也可以把当前工作目录换成其他的路径，如/tmp。改变工作目录的常见函数是chdir。(避免原父进程当前目录带来的一些麻烦)

  * **在子进程中调用umask()重设文件权限掩码为0**

    * 文件权限掩码是指**屏蔽掉文件权限中的对应位**。比如，有个文件权限掩码是050，它就屏蔽了文件组拥有者的可读与可执行权限（就是说可读可执行权限均变为7）。**由于使用fork函数新建的子进程继承了父进程的文件权限掩码，这就给该子进程使用文件带来了诸多的麻烦。因此把文件权限掩码重设为0即清除掩码（权限为777），这样可以大大增强该守护进程的灵活性**。通常的使用方法为umask(0)。(相当于把权限开发)

  * **在子进程中close()不需要的文件描述符**

    * 同文件权限码一样，用fork函数新建的子进程会从父进程那里继承一些已经打开了的文件。这些被打开的文件可能永远不会被守护进程读写，但它们一样消耗系统资源，而且可能导致所在的文件系统无法卸下
    * 其实在上面的第二步之后，守护进程已经与所属的控制终端失去了联系。因此从终端输入的字符不可能达到守护进程，守护进程中用常规方法（如printf）输出的字符也不可能在终端上显示出来。所以，**文件描述符为0、1和2 的3个文件（常说的输入、输出和报错）**已经失去了存在的价值，也应被关闭（关闭失去价值的输入、输出、报错等对应的文件描述符）

  * **守护进程退出处理**

    * 当用户需要外部停止守护进程运行时，往往会使用 kill 命令停止该守护进程。所以，守护进程中需要编码来实现 kill 发出的signal信号处理，达到进程的正常退出

    * 想退出守护进程，只需给守护进程发送 SIGQUIT 信号即可

      ```shell
      sudo kill -3 PID 
      ```

* 步骤流程图

  <img src="imgs/os/daemon.png" alt="daemon" style="zoom:67%;" />

* 代码实现

  ```c++
  #include<stdio.h>
  #include<fcntl.h>
  #include<sys/types.h>
  #include<unistd.h>
  #include<stdlib.h>
  
  void create_daemon()
  {
  	pid_t pid;
  	/*(1)-----创建一个进程来用作守护进程-----*/
  	pid = fork();
  	if(pid == -1){
  		printf("fork error\n");
  		exit(1);
  	}
  	/*(1.1)-----------原父进程退出-------------*/
  	else if(pid){
  		exit(0);
  	}
   	/*(2)---setsid使子进程独立。摆脱会话控制、摆脱原进程组控制、摆脱终端控制----*/
  	if(-1 == setsid()){
  		printf("setsid error\n");
  		exit(1);
  	}
    	/*(3)---通过再次创建子进程结束当前进程，使进程不再是会话首进程来禁止进程重新打开控制终端----*/
  	pid = fork();
  	if(pid == -1){
  		printf("fork error\n");
  		exit(1);
  	}
  	else if(pid){
  		exit(0);
  	}
    	/*(4)---子进程中调用chdir()让根目录成为子进程工作目录----*/
  	chdir("/");
      
      /*(5)---重设文件掩码为0（将权限全部开放）----*/
  	umask(0);
      
  	/*(6)---关闭文件描述符(常说的输入，输出，报错3个文件)----*/
  	for(int i = 0; i < 3; ++i){
  		close(i);
  	}
      // 将标准输入、标准输出、标准错误输出重定向到/dev/null文件
      open("/dev/null", O_RDONLY);	// 0
      open("/dev/null", O_RDWR);		// 1
      open("/dev/null", O_RDWR);		// 2
  	
  	return;
  }
  ```

* Linux提供了完成同样功能的库函数

  ```c++
  #include<unistd.h>
  int daemon(int nochdir, int noclose);
  // nochdir参数用于指定是否改变工作目录，如果给它传递0，则工作目录被设置为"/"(根目录)，否则继续使用当前工作目录
  // noclose参数为0时，标准输入、标准输出、标准错误输出都被重定向到/dev/null文件，否则依然使用原来的设备
  // 成功返回0，失败返回-1并设置errno
  ```

# 系统监测

## CPU，内存，IO监控

* **ps**命令用于显示当前进程的状态(cpu利用率， 占用内存，状态，运行时间等)，类似于 windows 的任务管理器

  ```shell
  ps [options] [--help]
  
  -A 列出所有的进程
  -w 显示加宽可以显示较多的资讯
  -au 显示较详细的资讯
  -aux 显示所有包含其他使用者的进程
  -ajx 获得当前所有的进程及父进程和进程组id
  -axjf/-ejH 查看所有进程的层次关系
  -ef 显示所有命令，连带命令行
  ```

  -aux 显示格式`USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND`

  * USER: 行程拥有者
  * PID: pid
  * %CPU: 占用的 CPU 使用率
  * %MEM: 占用的记忆体使用率
  * VSZ: 占用的虚拟记忆体大小
  * RSS: 占用的记忆体大小
  * TTY: 终端的次要装置号码 (minor device number of tty)
  * STAT: 该行程的状态:
    - D: 无法中断的休眠状态 (通常 IO 的进程)
    - R: 正在执行中
    - S: 静止状态
    - T: 暂停执行
    - Z: 不存在但暂时无法消除（僵尸进程）
    - W: 没有足够的记忆体分页可分配
    - <: 高优先序的行程
    - N: 低优先序的行程
    - L: 有记忆体分页分配并锁在记忆体内 (实时系统或捱A I/O)
  * START: 行程开始时间
  * TIME: 执行的时间
  * COMMAND:所执行的指令

* **vmstat**  可以展现给定时间间隔的服务器的状态值，包括服务器的**CPU使用率，内存使用，虚拟内存交换情况，IO读写情况**

  * 相比top，vmstat可以看到**整个机器**的CPU,内存,IO的使用情况，**而不是**单单看到**各个进程**的CPU使用率和内存使用率

  ```shell
  -a：显示活跃和非活跃内存
  -f：显示从系统启动至今的fork数量 。
  -m：显示slabinfo
  -n：只在开始时显示一次各字段名称。
  -s：显示内存相关统计信息及多种系统活动数量。
  delay：刷新时间间隔。如果不指定，只显示一条结果。
  count：刷新次数。如果不指定刷新次数，但指定了刷新时间间隔，这时刷新次数为无穷。
  -d：显示磁盘相关统计信息。
  -p：显示指定磁盘分区统计信息
  -S：使用指定单位显示。参数有 k 、K 、m 、M ，分别代表1000、1024、1000000、1048576字节（byte）。默认单位为K（1024 bytes）
  -V：显示vmstat版本信息
  ```

* **top** 实时监控系统状态，查看内存，端口 ，io访问量，读写速率

  ```shell
  top [-] [d delay] [q] [c] [S] [s] [i] [n] [b]
  d : 改变显示的更新速度，或是在交谈式指令列( interactive command)按 s
  q : 没有任何延迟的显示速度，如果使用者是有 superuser 的权限，则 top 将会以最高的优先序执行
  c : 切换显示模式，共有两种模式，一是只显示执行档的名称，另一种是显示完整的路径与名称
  S : 累积模式，会将己完成或消失的子行程 ( dead child process ) 的 CPU time 累积起来
  s : 安全模式，将交谈式指令取消, 避免潜在的危机
  i : 不显示任何闲置 (idle) 或无用 (zombie) 的行程
  n : 更新的次数，完成后将会退出 top
  b : 批次档模式，搭配 "n" 参数一起使用，可以用来将 top 的结果输出到档案内
  ```

  * top参数：load average，cpu使用率怎么算
    * 系统负载（System Load）是系统CPU繁忙程度的度量，即有多少进程在等待被CPU调度（**进程等待队列的长度**）
    * load average：1.97,2.14,2.99 来举例：
      - 第一位1.97：表示最近1分钟平均负载
      - 第二位2.14：表示最近5分钟平均负载
      - 第三位2.99：表示最近15分钟平均负载
    * Load < 0.7时：系统很闲，马路上没什么车，要考虑多部署一些服务
    * 0.7 < Load < 1时：系统状态不错，马路可以轻松应对
    * Load == 1时：系统马上要处理不多来了，赶紧找一下原因
    * Load > 1时：马路已经非常繁忙了，进入马路的每辆汽车都要无法很快的运行
    * 通常我们先看15分钟load，如果load很高，再看1分钟和5分钟负载，查看是否有下降趋势
    * 1分钟负载值 > 1，那么我们不用担心，但是如果15分钟负载都超过1，我们要赶紧看看发生了什么事情。所以我们要根据实际情况查看这三个值
    * "load average"一共返回三个平均值：1分钟系统负荷、5分钟系统负荷，15分钟系统负荷；
    * 如果只有1分钟的系统负荷大于1.0，其他两个时间段都小于1.0，这表明只是暂时现象，问题不大
    * 如果15分钟内，平均系统负荷大于1.0（调整CPU核心数之后），表明问题持续存在，不是暂时现象。所以，你应该主要观察"15分钟系统负荷"，将它作为电脑正常运行的指标。
    * 结合具体情况具体分析（单核）：
      - 1分钟Load>1，5分钟Load<1，15分钟Load<1：短期内繁忙，中长期空闲，初步判断是一个“抖动”，或者是“拥塞前兆”
      - 1分钟Load>1，5分钟Load>1，15分钟Load<1：短期内繁忙，中期内紧张，很可能是一个“拥塞的开始”
      - 1分钟Load>1，5分钟Load>1，15分钟Load>1：短、中、长期都繁忙，系统“正在拥塞”
      - 1分钟Load<1，5分钟Load>1，15分钟Load>1：短期内空闲，中、长期繁忙，不用紧张，系统“拥塞正在好转”

* **mpstat** 实时监测多处理器系统上的**每个CPU的使用情况**

* **iostat** 主要用于输出磁盘IO 和 CPU的统计信息

  ```shell
  iostat [选项] [<时间间隔>] [<次数>]
  -c： 显示CPU使用情况
  -d： 显示磁盘使用情况
  -N： 显示磁盘阵列(LVM) 信息
  -n： 显示NFS 使用情况
  -k： 以 KB 为单位显示
  -m： 以 M 为单位显示
  -t： 报告每秒向终端读取和写入的字符数和CPU的信息
  -V： 显示版本信息
  -x： 显示详细信息
  -p：[磁盘] 显示磁盘和分区的情况
  ```

* **free命令用于显示内存状态**，显示内存的使用情况，包括实体内存，虚拟的交换文件内存，共享内存区段，以及系统核心使用的缓冲区等

  ```shell
  free [-bkmotV][-s <间隔秒数>]
  -b 　以Byte为单位显示内存使用情况。
  -k 　以KB为单位显示内存使用情况。
  -m 　以MB为单位显示内存使用情况。
  -h 　以合适的单位显示内存使用情况，最大为三位数，自动计算对应的单位值。单位有B = bytes
       K = kilos
       M = megas
       G = gigas
       T = teras
  -o 　不显示缓冲区调节列。
  -s<间隔秒数> 　持续观察内存使用状况。
  -t 　显示内存总和列。
  -V 　显示版本信息。
  ```

## 网络监控

* **tcpdump root权限下使用的抓包工具，只能抓取流经本机的数据包**

  ```shell
  tcpdump [-adeflnNOpqStvx][-c<数据包数目>][-dd][-ddd][-F<表达文件>][-i<网络界面>][-r<数据包文件>][-s<数据包大小>][-tt][-T<数据包类型>][-vv][-w<数据包文件>][输出数据栏位]
  -a 尝试将网络和广播地址转换成名称。
  -c <数据包数目> 收到指定的数据包数目后，就停止进行倾倒操作。
  -d 把编译过的数据包编码转换成可阅读的格式，并倾倒到标准输出。
  -dd 把编译过的数据包编码转换成C语言的格式，并倾倒到标准输出。
  -ddd 把编译过的数据包编码转换成十进制数字的格式，并倾倒到标准输出。
  -e 在每列倾倒资料上显示连接层级的文件头。
  -f 用数字显示网际网络地址。
  -F<表达文件> 指定内含表达方式的文件。
  -i<网络界面> 使用指定的网络截面送出数据包。
  -l 使用标准输出列的缓冲区。
  -n 不把主机的网络地址转换成名字。
  -N 不列出域名。
  -O 不将数据包编码最佳化。
  -p 不让网络界面进入混杂模式。
  -q 快速输出，仅列出少数的传输协议信息。
  -r <数据包文件> 从指定的文件读取数据包数据。
  -s<数据包大小> 设置每个数据包的大小。
  -S 用绝对而非相对数值列出TCP关联数。
  -t 在每列倾倒资料上不显示时间戳记。
  -tt 在每列倾倒资料上显示未经格式化的时间戳记。
  -T <数据包类型> 强制将表达方式所指定的数据包转译成设置的数据包类型。
  -v 详细显示指令执行过程。
  -vv 更详细显示指令执行过程。
  -x 用十六进制字码列出数据包资料。
  -w <数据包文件> 把数据包数据写入指定的文件。
  ```

* **netstat 用于显示网络状态**，可以打印本地网卡接口上的全部链接 路由表信息 网卡接口信息等

  ```shell
  -a或--all 显示所有连线中的Socket。
  -A<网络类型>或--<网络类型> 列出该网络类型连线中的相关地址。
  -c或--continuous 持续列出网络状态。
  -C或--cache 显示路由器配置的快取信息。
  -e或--extend 显示网络其他相关信息。
  -F或--fib 显示路由缓存。
  -g或--groups 显示多重广播功能群组组员名单。
  -h或--help 在线帮助。
  -i或--interfaces 显示网络界面信息表单。
  -l或--listening 显示监控中的服务器的Socket。
  -M或--masquerade 显示伪装的网络连线。
  -n或--numeric 直接使用IP地址，而不通过域名服务器。
  -N或--netlink或--symbolic 显示网络硬件外围设备的符号连接名称。
  -o或--timers 显示计时器。
  -p或--programs 显示正在使用Socket的程序识别码和程序名称。
  -r或--route 显示Routing Table。
  -s或--statistics 显示网络工作信息统计表。
  -t或--tcp 显示TCP传输协议的连线状况。
  -u或--udp 显示UDP传输协议的连线状况。
  -v或--verbose 显示指令执行过程。
  -V或--version 显示版本信息。
  -w或--raw 显示RAW传输协议的连线状况。
  -x或--unix 此参数的效果和指定"
  -A unix"参数相同。
  --ip或--inet 此参数的效果和指定"-A inet"参数相同。
  ```

* **ifstat** 简单的网络流量监测工具

  ```shell
  -a 监测系统上的所有网卡接口
  -i 指定要监测的网卡接口
  -t 在每行输出信息前加上时间戳
  ```

* **iptables 对Linux系统中通信的数据包进行一定的检测，达到防火墙的目的**

* iptables 查看端口流量

  * 输入监控

    ```shell
    # 下面示例是监控目标端口是8080的输入流量 --dport(destination port 的缩写)
    iptables -A INPUT -p tcp --dport 8080
    ```

  * 输出监控

    ```shell
    # 下面示例是监控来源端口是8080的输出流量 --sport(source port 的缩写)
    iptables -A OUTPUT -p tcp --sport 8080
    ```

  * **查看统计数据**

    ```
    iptable -L -v -n -x
    
    Chain INPUT (policy ACCEPT 202 packets, 25187 bytes)
      pkts   bytes target   prot opt in   out   source        destination     
       18   2885      tcp -- *   *    0.0.0.0/0      0.0.0.0/0      tcp dpt:8080
    
    Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
      pkts   bytes target   prot opt in   out   source        destination     
    
    Chain OUTPUT (policy ACCEPT 184 packets, 45774 bytes)
      pkts   bytes target   prot opt in   out   source        destination     
       12   8240      tcp -- *   *    0.0.0.0/0      0.0.0.0/0      tcp spt:8080
    
    ```

  * **重置统计数据**

    ```shell
    # 重置所有输入端口
    Iptable -Z INPUT
    # 重置所有输出端口
    Iptable -Z OUTPUT
    ```

  * **移除统计端口**

    ```shell
    # 移除输入端口
    iptables -D INPUT -p tcp --dport 8080
    # 移除输出端口
    iptables -D OUTPUT -p tcp --sport 8080
    ```

* **lsof**查看一个进程打开了哪些文件，文件被哪些进程使用

  ```shell
  -p 18400 （按照进程ID查看）
  -d type (按照FD的类型查看）
  -i 4 (查看进程打开的网络连接，使用IPV4协议）
  -i:80 (查看进程打开的网络连接，端口号为80）
  -i @127.0.0.1 (查看进程打开的网络连接，IP为127.0.0.1）
  lsof file-name：查看文件对应的进程
  ```


## 监测消息列表、共享内存和信号量的信息

* **ipcs**：用于报告Linux中进程间通信设施的状态

  ```shell
  ipcs [resource ...] [output-format]ipcs [resource] -i 
  <id>选项：
  -i, --id <id>  打印由 id 标识的资源的详细信息
  -h, --help     显示此帮助并退出
  -V, --version  输出版本信息并退出
  资源选项:
  -m, --shmems      共享内存段
  -q, --queues      消息队列
  -s, --semaphores  信号量
  -a, --all         全部(默认)
  输出格式：
  -t, --time        显示附加、脱离和更改时间
  -p, --pid         显示 PID 的创建者和最后操作
  -c, --creator     显示创建者和拥有者
  -l, --limits      显示资源限制
  -u, --summary     显示状态摘要    
  --human       以易读格式显示大小
  -b, --bytes       以字节数显示大小
  ```

* **ipcrm**：删除一个或更多的消息队列、信号量集或者共享内存标识，同时会将与ipc对象相关链的数据也一起移除

  ```shell
  ipcrm [options]ipcrm <shm|msg|sem> <id> [...]
  -m, --shmem-id <id>        按 id 号移除共享内存段
  -M, --shmem-key <键>       按键值移除共享内存段
  -q, --queue-id <id>        按 id 号移除消息队列
  -Q, --queue-key <键>       按键值移除消息队列
  -s, --semaphore-id <id>    按 id 号移除信号量
  -S, --semaphore-key <键>  按键值移除信号量
  -a, --all[=<shm|msg|sem>]  全部移除
  -v, --verbose              解释正在进行的操作
  -h, --help     显示此帮助并退出
  -V, --version  输出版本信息并退出
  ```

* [**ulimit**](https://blog.csdn.net/FreeApe/article/details/101058393) 

  * shell内建指令，可用来控制shell执行程序的资源

  * 设置项仅在当前shell作用(类似`export`命令，永久生效可以写入相关配置文件)

  * 写入`~/.profile或~/.bashrc`**只对当前用户持久性生效**

  * 写入`/etc/security/limits.conf`可针对性配置，**系统级持久性生效**

    ```shell
    ulimit [-aHS][-c <core文件上限>][-d <数据节区大小>][-f <文件大小>][-m <内存大小>][-n <文件数目>][-p <缓冲区大小>][-s <堆叠大小>][-t <CPU时间>][-u <程序数目>][-v <虚拟内存大小>]
    
    -a 　显示目前资源限制的设定。
    -c <core文件上限> 　设定core文件的最大值，单位为区块。
    -d <数据节区大小> 　程序数据节区的最大值，单位为KB。
    -f <文件大小> 　shell所能建立的最大文件，单位为区块。
    -H 　设定资源的硬性限制，也就是管理员所设下的限制。
    -m <内存大小> 　指定可使用内存的上限，单位为KB。
    -n <文件数目> 　指定同一时间最多可开启的文件数。
    -p <缓冲区大小> 　指定管道缓冲区的大小，单位512字节。
    -s <进程栈大小> 　指定进程栈的上限，单位为KB。
    -S 　设定资源的弹性限制。
    -t <CPU时间> 　指定CPU使用时间的上限，单位为秒。
    -u <程序数目> 　用户最多可开启的程序数目。
    -v <虚拟内存大小> 　指定可使用的虚拟内存上限，单位为KB。
    ```

* linux的fd数量限制查询

  * 所有进程允许打开的最大fd数量

  ```shell
  cat /proc/sys/fs/file-max   // 9223372036854775807
  ```

  * 所有进程已经打开的fd数量及允许的最大数量

  ```shell
  cat /proc/sys/fs/file-nr
  ```

  * 单个进程允许打开的最大fd数量

  ```shell
  ulimit -n
  ```

  * 单个进程（例如进程id为5454）已经打开的fd

  ```shell
  ls -l /proc/5454/fd/lsof -p 5454
  ```

## 进程/线程绑定cpu

* 意义
  * 一是现代服务器大多采用NUMA多处理器架构，一台服务器会安装多颗处理器（称为NUMA节点），而NUMA架构各节点资源较为独立的设计，决定了在不同NUMA节点共享数据的成本高昂，因此尽量将数据交互较为频繁的程序绑定在同一NUMA节点上是很重要的
  * 二是进程/线程**如果从一个核心切换至另一个核心上运行，需要面临上下文切换、缓存失效等问题，成本也很高**。在对性能要求较高的软件中，这已经是造成时延抖动的一大来源之一

* 进程/线程绑定

  * 在Linux系统下，进程都有一个CPU亲和力属性（affinity），通过以下命令可以查询

    ```bash
    # 查看cpu信息
    cat /proc/cpuinfo | grep processor
    processor	: 0
    # 查看某个进程可以运行的cpu
    taskset -pc 26419 
    pid 26419's current affinity list: 0
    ```

    以上查询结果的含义是，进程id为26419的进程，可以在0号CPU上运行，默认情况下，进程是可以在任意一个核心上运行的

  * 绑定cpu

    ```
    taskset -pc 0,1,2... pid	# 表示将pid进程绑定到0，1，2...号cpu执行
    ```

  * taskset命令依然适用于线程

  * 不能通过taskset+程序名直接启动程序并实现各个线程**绑定不同核心**了，该命令只能支持到进程级别，也就是说它会把进程下的所有线程都设置为相同的亲和度

  * 如果需要将同一进程中线程绑定到不同cpu，需要手动taskset tid：可以编写脚本启动并获取线程id，再多次调用taskset，指定线程id，将其绑定到不同核心上即可

* 编程绑定

  * sched_getaffinity、sched_setaffinity
  * pthread_setaffinity_np

## 查看某个服务是否启动

* 方法一：查看redis服务进程情况，这里拿redis服务举例,其他服务查询更改名字即可

  ```
  ps -ef|grep redis
  ```

* 方法二：查看6379（为redis的端口号）端口号是否被占用

  ```
  lsof -i :6379
  ```

* 方法三：显示tcp的端口和进程等相关情况

  ```
  netstat -tnlp
  ```

# 编译调试

## C++进程内存分布

<img src="imgs/os/mem_align.png" alt="mem_align" style="zoom:90%;" />

## ar, nm, ldd, readelf, objdump

* ar：建立或修改备存文件，或是从备存文件中抽取文件

  ```shell
  ar[-dmpqrtx][cfosSuvV][a<成员文件>][b<成员文件>][i<成员文件>][备存文件][成员文件]
  必要参数：
  -d 　删除备存文件中的成员文件。
  -m 　变更成员文件在备存文件中的次序。
  -p 　显示备存文件中的成员文件内容。
  -q 　将文件附加在备存文件末端。
  -r 　将文件插入备存文件中。
  -t 　显示备存文件中所包含的文件。  # 常用
  -x 　自备存文件中取出成员文件。
  选项参数：
  a <成员文件> 　将文件插入备存文件中指定的成员文件之后。
  b <成员文件> 　将文件插入备存文件中指定的成员文件之前。
  c 　建立备存文件。
  f 　为避免过长的文件名不兼容于其他系统的ar指令指令，因此可利用此参数，截掉要放入备存文件中过长的成员文件名称。
  i <成员文件> 　将文件插入备存文件中指定的成员文件之前。
  o 　保留备存文件中文件的日期。
  s 　若备存文件中包含了对象模式，可利用此参数建立备存文件的符号表。
  S 　不产生符号表。
  u 　只将日期较新文件插入备存文件中。
  v 　程序执行时显示详细的信息。
  V 　显示版本信息。
  ```

* nm：nm用来列出目标文件的**符号表清单**

* ldd

  * 首先ldd不是一个可执行程序，而只是一个shell脚本
  * ldd能够显示**可执行模块的dependency**，其原理是通过设置一系列的环境变量，如下：LD_TRACE_LOADED_OBJECTS、LD_WARN、LD_BIND_NOW、LD_LIBRARY_VERSION、LD_VERBOSE等
  * ldd显示**可执行模块**的dependency的工作原理，其实质是通过ld-linux.so（elf动态库的装载器）来实现

* **readelf**：一般用于查看**ELF格式的文件信息**，常见的文件如在Linux上的**可执行文件**，**动态库(*.so)或者静态库(*.a) 等包含ELF格式**的文件

  * **系统里的目标文件是按照特定的目标文件格式来组织的，各个系统的目标文件格式都不相同**

  * ELF(Executable and Linking Format)**可执行可链接格式**是一种对象文件的格式，**用于定义不同类型的对象文件(Object files)中都放了什么东西、以及都以什么样的格式去放这些东西**。它自最早在 System V 系统上出现后，被 xNIX 世界所广泛接受，作为缺省的二进制文件格式来使用。可以说，ELF是构成众多xNIX系统的基础之一

    * ELF文件有三种类型：
      * **可重定位的对象文件**(Relocatable file) 由汇编器汇编生成的 .o 文件	
      * **可执行的对象文件**(Executable file) 可执行应用程序
      * **可被共享的对象文件**(Shared object file) 动态库文件，也即 .so 文件

    * 在Unix下使用**readelf命令来显示可执行程序的信息，功能与objdump相似，但是显示的更加具体**

  * ELF格式的文件在Linux系统下有.axf、 .bin、 .elf、 .o、 .prx、 .puff、 .ko、 .mod和.so等等

* objdump是用来**显示目标文件相关信息**的

## 编译链接和载入

* 编译：将预处理生成的文件，经过**语法分析、词法分析、语义分析、以及优化后编译成若干个目标模块**。可以理解为将高级语言翻译为计算机可以理解的二进制代码，即机器语言
* 链接：由链接程序将编译后形成的**一组目标模块**以及他们所需要的**库函数**链接在一起，形成一个完整的**载入模型**。链接主要解决**模块间的相互引用问题**，分为**地址和空间分配**，**符号解析**和**重定位**几个步骤。**在编译阶段生成目标文件时，会暂时搁置这些外部引用**，而这些外部引用就是在链接时确定的，链接器在链接时，会**根据符号名称去相应模块中寻找对应的符号**，**待符号确定后，链接器会重写之前哪些未确定的符号的地址**，这个过程就是重定位
* 载入：由载入程序将载入模块载入内存

## 动态链接和静态链接的区别

* 静态链接以一组可重定位目标文件为输入，文件由各种不同的代码和数据节组成，通过符号解析和重定位生成一个完全链接的可以加载和运行的可执行文件
* 静态链接有一些明显的缺点，**一是如果需要更新一个库，需要重新编译和链接库文件**。二是对于一些标准的函数，**如果将这些代码复制到每个程序运行的文本段中，会对存储器的资源造成很大的浪费**
* 共享库就是为解决静态链接问题而生，共享库是一个目标模块。在运行时，可以加载到任意存储器地址，并和一个在存储器中的程序链接起来。这个过程称为动态链接。共享库在unix下通常使用.so后缀，window下为dll
* 共享库使用两种方式共享，一是一个库只有一个so文件，**所有引用该库的执行程序共享这个文件的代码和数据**。**二是一个共享库的.text节的一个副本可以被不同的进程共享**
* 注意在整个程序的链接过程中，**链接器只是拷贝了一些重定位和符号信息**。**在程序加载（execve）时才会解析so文件中代码和数据的引用**

## 动态库和静态库的创建与使用

* 库的名字由 lib+name+后缀，静态库的后缀为.a 动态库的后缀为.so 

* 动态库的创建步骤

  * 将.c文件全部编译为.o文件

  * 在Shell下输入命令，创建动态库：

    ```shell
    gcc -shared –fPIC –o libname.so sourcefile.c
    -shared 该选项指定生成动态连接库（让连接器生成T类型的导出符号表，有时候也生成弱连接W类型的导出符号），不用该标志外部程序无法连接。相当于一个可执行文件
    -fPIC：表示编译为位置独立的代码，不用此选项的话编译后的代码是位置相关的所以动态载入时是通过代码拷贝的方式来满足不同进程的需要，而不能达到真正代码段共享的目的
    ```

  * 使用动态库

    ```shell
    gcc -o file file.c -L. -lname
    -L.：表示要链接的库在当前目录中
    ```

  * **动态链接时、执行时搜索路径顺序**

    * 编译目标代码时，`-L`指定的动态库搜索路径
    * 环境变量`LD_LIBRARY_PATH`指定的动态库搜索路径
    * 配置文件`/etc/ld.so.conf`中指定的动态库搜索路径
    * 默认的动态库搜索路径`/lib`，`/usr/lib`

* 静态库的创建步骤

  * 将.c文件全部编译为.o文件

  * 在Shell下输入命令，创建静态库： 

    ```shell
    ar rcs libname.a 目标文件1 目标文件2...
    ```

* 使用静态库

  ```shell
   gcc  -o file file.c –L. -lname 
   # -L 为静态库路径 
   # -l 为是链接到库的名字（可以简写库的名字）
  ```

* **静态库链接时搜索路径顺序**

  * ld会去找GCC命令中的参数`-L`

  * 再找gcc的环境变量`LIBRARY_PATH`
  * 再找内定目录 `/lib`，`/usr/lib`， `/usr/local/lib` 这是当初compile gcc时写在程序内的

## gcc和g++的异同

* 均属于the GNU Compiler Collection，gcc是鼻祖，后来才有了g++

  ```c++
  g++ == gcc -xc++ -lstdc++ -shared-libgcc	// gcc后面两项都是链接选项，表示g++要相比gcc链接其他库函数
  ```

* 不同

  * **g++会自动链接C++标准库**，比如algorithm，string，vector等

  * gcc会根据文件后缀(.c,.cpp)自动识别是C文件还是C++文件，g++均认为是C++文件

  * gcc编译C文件少很多宏定义，gcc编译C++会多一些宏定义

    ```c
    #define __GXX_WEAK__ 1
    #define __cplusplus 1
    #define __DEPRECATED 1
    #define __GNUG__ 4
    #define __EXCEPTIONS 1
    #define __private_extern__ extern
    ```

## gdb调试

### 基本命令

* [详见1](https://blog.csdn.net/qq_26399665/article/details/81165684)
* [详见2](https://blog.csdn.net/weixin_45596153/article/details/101453983)

### gdb调试多线程

```c++
info threads //显示当前可调式的所有线程 
thread ID //切换当前调试的线程为指定ID的线程
thread apply all command //所以的线程都执行command命令
thread apply ID1,ID2.... command //指定线程执行command命令
set scheduler-locking off|on|step： 	// 在使用step或continue命令调试当前被调试线程的时候，其他线程也是同时执行的，如果我们只想要被调试的线程执行，而其他线程停止等待，那就要锁定要调试的线程，只让它运行。 	
    //off:不锁定任何线程，所有线程都执行。     
    //on:只有当前被调试的线程会执行。 　　 
    //step:阻止其他线程在当前线程单步调试的时候抢占当前线程。只有当next、continue、util以及finish的时候，其他线程才会获得重新运行的show scheduler-locking： 查看当前锁定线程的模式i threads 实现线程间切换
```

### gdb调试多进程

```c++
// 设置方法
set follow-fork-mode [parent][child] 
set detach-on-fork [on|off]  
// 查看上述两个属性的值
show follow-fork-mode //查看系统默认的模式
show detach-on-fork
/* 
	parent                   on               只调试主进程（GDB默认）
	child                    on               只调试子进程
	parent                   off              同时调试两个进程，gdb跟主进程，子进程block在fork位置
	child                    off              同时调试两个进程，gdb跟子进程，主进程block在fork位置
*/

// 查询正在调试的进程
info inferiors  //查询正在调试的进程
inferior 进程编号 // 切换调试的进程
add-inferior [-copies n] [-exec executable] //添加新的调试进程
detach inferior [进程编号] //释放掉 
kill inferior [进程编号] 
remove-inferior [进程编号] //删除该进程
set schedule-multiple 
set print interior-events on/off
```

### Core dump(核心转储)概念及调试方法

* 程序由于各种异常或者bug导致在运行过程中异常退出或者中止，并且在满足一定条件下会产生一个叫core的文件，通常情况下，core文件会包含了程序运行时的**内存，寄存器状态，堆栈指针，内存管理信息还有各种函数调用堆栈信息**等，我们可以理解为是程序工作当前状态存储生成第一个文件，许多的程序出错的时候都会产生一个core文件，通过工具分析这个文件，我们可以定位到程序异常退出的时候对应的堆栈调用等信息，找出问题所在并进行及时解决
* **产生core dump的几种情况**
  * **内存访问越界**
  * **多线程程序使用了线程不安全的函数**：应该使用**可重入的函数**
  * **多线程读写的数据未加锁保护**：对于会被多个线程同时访问的全局数据，应该注意加锁保护，否则很容易造成core dump
  * **非法指针**：使用空指针，随意使用指针转换
  * **堆栈溢出**：不要使用大的局部变量（因为局部变量都分配在栈上），这样容易造成堆栈溢出，破坏系统的栈和堆结构，导致出现莫名其妙的错误
* `ulimit  -c` 可以设置core文件的大小，如果**这个值为0.则不会产生core文件**，这个**值太小，则core文件也不会产生**，因为core文件一般都比较大
* 使用`ulimit  -c unlimited`来设置无限大，则**任意情况下都会产生core文件**
* 控制core文件保存位置和文件名格式
  * echo "/var/core_log/core-%e-%p-%t" > /proc/sys/kernel/core_pattern，这是临时的，这个也是动态加载和生成的
  * 永久修改在/etc/sysctl.conf文件中，在该文件的最后加上两行：sysctl -w kernel.core_pattern=/var/core_log/core-%e-%t-%p 和kernel.core_uses_pid = 0，最后执行sysctl –p生效
  * 以下是参数列表:
    * %p - insert pid into filename 当前pid(进程id)
    * %u - insert current uid into filename 当前uid(用户id)
    * %g - insert current gid into filename 当前gid(用户组id)
    * %s - insert signal that caused the coredump into the filename 导致产生core的信号
    * %t - insert UNIX time that the coredump occurred into filename core文件生成时的unix时间
    * %h - insert hostname where the coredump happened into filename 主机名
    * %e - insert coredumping executable name into filename 导致产生core的命令名/文件名
* 调试
  * gcc -o test -test.c -g
  * 运行程序./test，产生core文件在/var/core_log目录中
  * 进入/var/core_log，gdb调试core文件：gdb /path/test core-test-time-pid
  * gdb运行直到发生coredump，此时利用bt指令查看堆栈信息定位发生点

### [**内存泄漏调试方法**](https://blog.csdn.net/weixin_36343850/article/details/77856051)

* 内存泄漏的概念
  * 内存泄漏是指由于疏忽或错误造成了程序未能释放掉不再使用的内存的情况。内存泄漏并非指内存在物理上的消失，而是**应用程序分配某段内存后，由于设计错误，失去了对该段内存的控制，因而造成了内存的浪费**
  * 内存泄漏是指堆内存的泄漏。堆内存是指程序从堆中分配的、大小任意的(内存块的大小可以在程序运行期决定)、使用完后必须显示释放的内存。应用程序一般使用malloc、realloc、new等函数从堆中分配到一块内存，使用完后，程序必须负责相应的调用free或delete释放该内存块。否则，这块内存就不能被再次被申请使用，我们就说这块内存泄漏了
* 内存泄漏分类
  * 堆内存泄漏 （Heap leak）。对内存指的是程序运行中根据需要分配通过malloc,realloc new等从堆中分配的一块内存，再是完成后必须通过调用对应的 free或者delete 删掉。如果程序的设计的错误导致这部分内存没有被释放，那么此后这块内存将不会被使用，就会产生Heap Leak
  * 系统资源泄露（Resource Leak）。主要指程序使用系统分配的资源比如 Bitmap,handle ,SOCKET等没有使用相应的函数释放掉，导致系统资源的浪费，严重可导致系统效能降低，系统运行不稳定
  * 没有将基类的析构函数定义为虚函数。当基类指针指向子类对象时，如果基类的析构函数不是virtual，那么子类的析构函数将不会被调用，子类的资源没有正确是释放，因此造成内存泄露

### 什么时候发生段错误？

* MMU在做逻辑地址到物理地址的转换时发生2次检查
  * 检查逻辑地址是否在某个已定义的内存映射区域，这一步通过和mm_struct中，mmap指针所记录的vm_area_struct链表中的每个每个节点所限定的虚拟内存区域比较实现。vm_area_struct结构中的vm_start和vm_end成员记录该节点所定义的虚拟内存区域的起始/结束地址（逻辑地址）。**如果要访问的地址不在任何一个区域中，则说明是一个非法的地址**。Linux在搜索vm_area_struct是，不是使用链表，而是使用树结构加速查找速度
  * MMU得到该地址的页表项，检查页表项中的权限信息，如果**操作（读/写）与权限不符**，则触发保护异常(**使用野指针，试图修改字符串常量**等)

* 上述两种操作**都会导致段错误**

* core文件中是什么，gdb调试core文件

  * **在一个程序崩溃时，它一般会在指定目录下生成一个core文件**。core文件**仅仅是一个内存映象(同时加上调试信息)，主要是用来调试的**。通过core文件调试步骤：
    * ulimit -c unlimted（**打开core，默认没有打开**）
    * 运行./a.out（编译的时候加调试选项-g） **死锁阻塞，Ctrl+\ 产生core dump**
    * gdb ./a.out core.xxx

### gdb调试死锁

* **借助 Core Dump**。在程序莫名其妙down掉了，此时操作系统会把当前的内存状况存储在一个core 文件中，通过查看core文件就可以直观的程序是因为什么而垮掉了。有时候程序down了, 但是core文件却没有生成，core文件的生成跟你当前系统的环境设置有关系，可以用下面的语句设置一下，然后再运行程序便会生成core文件

  ```shell
  ulimit -c unlimited
  ```

* core文件生成的位置一般于运行程序的路径相同，文件名一般为“**core.进程号**”

* 在多线程调试中使用Core Dump：

  * 使用 kill 命令产生 core dump文件：kill -11 pid 产生core文件
  * 使用gdb工具打开core文件 gdb dead_lock_demo core
  * 打印堆栈信息 **thread apply all bt查看死锁位置**
  
* 另外gdb可跟踪运行中的程序，使用`attach pid`命令可直接attach到进程或者线程，bt查看其运行的栈信息，这种方式容易定位死锁的进程

### strace

- `strace -p pid`即可打印出进程的**系统调用信息，包括参数，返回值**等
- 它还能**调试进程接收到的信号情况**
- 频繁地系统调用会影响性能，这个命令可以用来调试性能及bug

## 在进程崩掉后如何追踪日志

* 进程日志

  * 在程序开发中，肯定会记录一些日志，而日志记录的好坏可以直接影响调试，进而影响程序的发布进程
  * 目前有很多的开源日志库，选择合适的即可。如log4cplus等
  * 日志等级较多时，**一般在运行时，只记录WARN级别以上信息**，**如果有实时调整日志级别，将非常有助于定位问题**。
    **实时打开DEBUG级别日志，能详细跟踪程序流程**
  * 如果不能实时调整，只能重启的方式，这时bug可能不易复现

* core文件

  * 生产环境可能不会产生core文件，这时日志的记录就尤其重要了
  * 测试环境建议先打开内核转储：ulimit -c unlimited，或者调试到配置里，不用每次开个终端都要设置一遍
  * **注意，core文件与编译时的-g选项结合使用，且编译时不要加-02，否则gdb会看不到调试信息**
  * 使用gdb -c core a.out调试，bt打印崩溃时的栈信息，基本可以发现出问题的代码行

  * 更多gdb的命令可以查看相关man page，基本介绍可以参考内核转储coredump简介

* dmesg
  * 打印内核日志信息

  * 系统级别相关的信息会存储在此处，如进程异常崩溃退出等，也会有记录

  * 当一个进程使用的内存较大时，会被操作系统kill掉，这样的信息就会记录在dmesg中

    ```
    [1892837.939243] Out of memory: Kill process 10735 (oomServer) score 952 or sacrifice child
    ```

  * 根据进程异常崩溃信息，可以反推崩溃位置

    ```
    [5596955.061423] traps: TrapServer[32530] trap divide error ip:4bb78a sp:7ff530ff7230 error:0 in TrapServer[400000+251000]
    ```

  * 根据ip位置，使用`addr2line -e TrapServer 4bb78a`可打印进程的行号

* linux系统日志

  * https://blog.csdn.net/Mikeoperfect/article/details/79234585
  * https://blog.csdn.net/dubendi/article/details/78782691

## CPU、内存占用过高问题定位

* https://blog.csdn.net/GR9527/article/details/108456151
* http://www.cxyzjd.com/article/qq_32273965/106749858
* 步骤：
  * 使用`top`定位到占用CPU高的进程PID   然后按`shift+p`按照CPU排序
  * 再用`ps -mp pid -o THREAD,tid,time`查询进程中,那个线程的cpu占用率高记住TID

## 其他

* 如何读取一个10G文件，cat一个10g文件会发生什么

  - 强行 cat 一个大文件会造成内存溢出，通常将**cat**命令和**split**命令混合使用。
  - 比如内存是250M， 那么将10G的文件切分成若干个250M的文件，然后文本查找

# 文件管理

* Linux目录结构

  <img src="imgs/os/dir.png" alt="dir" style="zoom:80%;" />

  




* 文件储存原理

  * 文件储存在硬盘上，硬盘的**最小存储单位叫做"扇区"**（Sector）。**每个扇区储存512字节**（相当于0.5KB）。
  * 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个"块"（block）。这种由多个扇区组成的"块"，是**文件存取的最小单位**。**"块"的大小，最常见的是4KB**，即**连续八个 sector组成一个 block**。
  * 文件数据都储存在"块"中，那么很显然，我们还必须**找到一个地方储存文件的元信息**，比如**文件的创建者、文件的创建日期、文件的大小**等等。**这种储存文件元信息的区域就叫做inode，中文译名为"索引节点"**。

* inode概念

  * 每一个文件都有对应的inode，里面包含了与该文件有关的一些信息

* inode内容

  * 文件的字节数
  * 文件拥有者的User ID
  * 文件的Group ID
  * 文件的读、写、执行权限
  * 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间
  * 链接数，即有多少文件名指向这个inode
  * 文件数据block的位置

* 可以用`stat`命令，查看某个文件的inode信息：`stat filename`

* inode的大小

  * inode也会消耗硬盘空间，所以硬盘格式化的时候，**操作系统自动将硬盘分成两个区域**。一个是**数据区**，存放文件数据；另一个是**inode区**（inode table），存放**inode所包含的信息**
  * 每个inode节点的大小，一般是128字节或256字节。inode节点的总数，在格式化时就给定，一般是**每1KB或每2KB就设置一个inode**。假定在一块1GB的硬盘中，每个inode节点的大小为128字节，每1KB就设置一个inode，那么inode table的大小就会达到128MB，占整块硬盘的12.8%。
  * 查看每个硬盘分区的inode总数和已经使用的数量，可以使用`df`命令：`df -i`
  * 查看每个inode节点的大小，可以用：`sudo dumpe2fs -h /dev/hda | grep "Inode size"`
  * 由于每个文件都必须有一个inode，因此有可能发生inode已经用光，但是硬盘还未存满的情况。这时，就无法在硬盘上创建新文件

* **inode号码**

  * 每个inode都有一个号码，操作系统用inode号码来识别不同的文件
  * 这里值得重复一遍，Unix/Linux系统内部不使用文件名，而使用inode号码来识别文件。对于系统来说，文件名只是inode号码便于识别的别称或者绰号
  * 表面上，用户通过文件名，打开文件。实际上，系统内部这个过程**分成三步**：首先，**系统找到这个文件名对应的inode号码**；其次，**通过inode号码，获取inode信息**；最后，**根据inode信息，找到文件数据所在的block，读出数据**
  * 查看文件名对应的inode号码：`ls -i filename`

* **目录文件**

  * Unix/Linux系统中，目录（directory）也是一种文件。**打开目录，实际上就是打开目录文件**
  * 目录文件的结构非常简单，就是一系列目录项（dirent）的列表。**每个目录项，由两部分组成：所包含文件的文件名，以及该文件名对应的inode号码**
  * 列出整个目录文件，即文件名和inode号码：`ls -i dir`
  * 要查看文件的详细信息，就必须根据inode号码，访问inode节点，读取信息。`ls -l`命令列出文件的详细信息
  * 目录权限
    * 理解了上面这些知识，就能理解目录的权限。目录文件的读权限（r）和写权限（w），都是针对目录文件本身，即**不同用户能以什么权限访问操作对该目录文件**。
    * 例如这里不同用户对tmp目录文件（d可以查出tmp是目录文件，d表示directory，即目录）分别为rwxr-xr-x，第一组的三个字符，即rwx，表示文件拥有者用户的对该文件的读写权限，第二组的三个字符，即r-x，表示文件拥有者用户所在的用户组里的其他用户对该文件的读写权限，第三组的三个字符，即r-x，表示文件拥有者用户所在的用户组以外的用户对该文件的读写权限。一个某个用户下运行的进程访问操作该目录文件只能以该用户所具有的对该目录文件的权限进行操作
    * 由于目录文件内**只有文件名和inode号码**，所以**如果只有读权限，只能获取文件名**，无法获取其他信息，因为其他信息都储存在inode节点中，而读取inode节点内的信息需要目录文件的执行权限（x）

* 硬链接

  * 一般情况下，文件名和inode号码是"一一对应"关系，每个inode号码对应一个文件名
  * **但是，Unix/Linux系统允许，多个文件名指向同一个inode号码。这意味着，可以用不同的文件名访问同样的内容；对文件内容进行修改，会影响到所有文件名；但是，删除一个文件名，不影响另一个文件名的访问。这种情况就被称为"硬链接"（hard link）。**
  * ln命令可以创建硬链接：`ln sourcefile destfile`
  * 运行上面这条命令以后，源文件与目标文件的inode号码相同，都指向同一个inode。**inode信息中有一项叫做"链接数"，记录指向该inode的文件名总数，**这时就会增加1。
  * **反过来，删除一个文件名，就会使得inode节点中的"链接数"减1。当这个值减到0，表明没有文件名指向这个inode，系统就会回收这个inode号码，以及其所对应block区域。**
  * 这里顺便说一下目录文件的"链接数"。创建目录时，默认会生成两个目录项："."和".."。前者的inode号码就是当前目录的inode号码，等同于当前目录的"硬链接"；后者的inode号码就是当前目录的父目录的inode号码，等同于父目录的"硬链接"。所以，任何一个目录的"硬链接"总数，总是等于2（某一目录的目录名和该目录的当前目录名）

* 软连接

  * 除了硬链接以外，还有一种特殊情况
  * **文件A和文件B的inode号码虽然不一样**，但是文件A的内容是文件B的路径。读取文件A时，系统会自动将访问者导向文件B。因此，无论打开哪一个文件，最终读取的都是文件B。这时，文件A就称为文件B的"软链接"（soft link）或者"符号链接（symbolic link）
  * 这意味着，**文件A依赖于文件B而存在，如果删除了文件B，打开文件A就会报错**："No such file or directory"。这是软链接与硬链接最大的不同：文件A指向文件B的文件名，而不是文件B的inode号码，文件B的inode"链接数"不会因此发生变化。
  * ln -s命令可以创建软链接：`ln -s sourcefile destfile`

* **inode的特殊作用**

  * 由于inode号码与文件名分离，这种机制导致了一些Unix/Linux系统特有的现象
    * 有时，**文件名包含特殊字符，无法正常删除**。这时，**直接删除inode节点，就能起到删除文件的作用**
    * **移动文件或重命名文件，只是改变文件名，不影响inode号码**
    * 打开一个文件以后，系统就以inode号码来识别这个文件，不再考虑文件名。因此，通常来说，系统无法从inode号码得知文件名
  * 第3点使得软件更新变得简单，可以在不关闭软件的情况下进行更新，不需要重启。因为系统通过inode号码，识别运行中的文件，不通过文件名。更新的时候，新版文件以同样的文件名，生成一个新的inode，不会影响到运行中的文件。等到下一次运行这个软件的时候，文件名就自动指向新版文件，旧版文件的inode则被回收

# 零拷贝技术

* 概念

  * **零拷贝（ zero-copy ）**技术可以有效地改善数据传输的性能，在内核驱动程序（比如网络堆栈或者磁盘存储驱动程序）处理 I/O 数据的时候，零拷贝技术可以在某种程度上减少甚至完全避免不必要 CPU 数据拷贝操作。
  * **零拷贝就是一种避免 CPU 将数据从一块存储拷贝到另外一块存储的技术**。**针对操作系统中的设备驱动程序、文件系统以及网络协议堆栈而出现的各种零拷贝技术极大地提升了特定应用程序的性能，并且使得这些应用程序可以更加有效地利用系统资源**。这种性能的提升就是**通过在数据拷贝进行的同时，允许 CPU 执行其他的任务来实现的**
  * 概括如下
    * **避免数据拷贝**
      * **避免操作系统内核缓冲区之间进行数据拷贝操作**
      * **避免操作系统内核和用户应用程序地址空间这两者之间进行数据拷贝操作**
      * **用户应用程序可以避开操作系统直接访问硬件存储**
      * **数据传输尽量让 DMA 来做**
    * **综合目标**
      * **避免不必要的系统调用和上下文切换**
      * 需要拷贝的数据可以先被缓存起来
      * 对数据进行处理尽量让硬件来做

* 零拷贝使用场景

  * 在写一个服务端程序时（Web Server或者文件服务器），文件下载是一个基本功能。这时候服务端的任务是：将服务端主机磁盘中的文件不做修改地从已连接的socket发出去，我们通常用下面的代码完成：

    ```c++
    while((n = read(diskfd, buf, BUF_SIZE)) > 0)
       write(sockfd, buf , n);
    ```

  * 基本操作就是循环的从磁盘读入文件内容到缓冲区，再将缓冲区的内容发送到socket。但是由于Linux的I/O操作默认是缓冲I/O。这里面主要使用的也就是read和write两个系统调用，我们并不知道操作系统在其中做了什么。实际上在以上I/O操作中，发生了多次的数据拷贝。

  * 当应用程序访问某块数据时，操作系统首先会检查，是不是最近访问过此文件，文件内容是否缓存在内核缓冲区，如果是，操作系统则直接根据read系统调用提供的buf地址，将内核缓冲区的内容拷贝到buf所指定的用户空间缓冲区中去。如果不是，**操作系统则首先将磁盘上的数据拷贝的内核缓冲区，这一步目前主要依靠DMA来传输**，然后再把内核缓冲区上的内容拷贝到用户缓冲区中。接下来，write系统调用再把用户缓冲区的内容拷贝到网络堆栈相关的内核缓冲区中，最后socket再把内核缓冲区的内容发送到网卡上

    <img src="imgs/os/multi_copy.png" alt="multi_copy" style="zoom:80%;" />

  * 从上图中可以看出，共产生了**两次系统调用，四次数据拷贝**，即使使用了DMA来处理了与硬件的通讯，CPU仍然需要处理两次数据拷贝，与此同时，在用户态与内核态也发生了多次上下文切换，无疑也加重了CPU负担。在此过程中，我们没有对文件内容做任何修改，那么在内核空间和用户空间来回拷贝数据无疑就是一种浪费，而零拷贝主要就是为了解决这种低效性

  * 零拷贝主要的任务就是避免CPU将数据从一块存储拷贝到另外一块存储，主要就是利用各种零拷贝技术，避免让CPU做大量的数据拷贝任务，减少不必要的拷贝，或者让别的组件来做这一类简单的数据传输任务，让CPU解脱出来专注于别的任务。这样就可以让系统资源的利用更加有效

* 零拷贝技术使用

  * **使用mmap**

    * 减少拷贝次数的一种方法是调用mmap()来代替read调用：

      ```c++
      buf = mmap(diskfd, len);
      write(sockfd, buf, len);
      ```

    * 应用程序调用mmap()，**磁盘上的数据会通过DMA被拷贝的内核缓冲区**，**接着操作系统会把这段内核缓冲区与应用程序共享**，**这样就不需要把内核缓冲区的内容往用户空间拷贝**。应用程序**再调用write()**，**操作系统直接将内核缓冲区的内容拷贝到socket缓冲区中**，这一切都发生在内核态，最后，socket缓冲区再把数据发到网卡去

    <img src="imgs/os/mmap.png" alt="mmap" style="zoom:80%;" />

  * 使用mmap替代read很明显减少了一次拷贝，当拷贝数据量很大时，无疑提升了效率。**但是使用mmap是有代价的。当你使用mmap时，你可能会遇到一些隐藏的陷阱。例如，当你的程序map了一个文件，但是当这个文件被另一个进程截断(truncate)时, write系统调用会因为访问非法地址而被SIGBUS信号终止**。SIGBUS信号默认会杀死你的进程并产生一个coredump,如果你的服务器这样被中止了，那会产生一笔损失。通常可以使用以下解决方案避免这种问题：

    * **为SIGBUS信号建立信号处理程序**

      * 当遇到SIGBUS信号时，信号处理程序简单地返回，write系统调用在被中断之前会返回已经写入的字节数，并且errno会被设置成success，但是这是一种糟糕的处理办法，因为你并没有解决问题的实质核心

    * **使用文件租借锁**

      * 通常我们使用这种方法，在**文件描述符上使用租借锁**，我们为文件向内核申请一个租借锁，当其它进程想要截断这个文件时，内核会向我们发送一个实时的`RT_SIGNAL_LEASE`信号，告诉我们内核正在破坏你加持在文件上的读写锁。这样在程序访问非法内存并且被SIGBUS杀死之前，你的write系统调用会被中断。write会返回已经写入的字节数，并且置errno为success。我们应该在mmap文件之前加锁，并且在操作完文件后解锁：

        ```c++
        if(fcntl(diskfd, F_SETSIG, RT_SIGNAL_LEASE) == -1) {
            perror("kernel lease set signal");
            return -1;
        }
        /* l_type can be F_RDLCK F_WRLCK  加锁*/
        /* l_type can be  F_UNLCK 解锁*/
        if(fcntl(diskfd, F_SETLEASE, l_type)){
            perror("kernel lease set type");
            return -1;
        }
        ```

* 使用sendfile

  * sendfile原型

    ```c++
    #include<sys/sendfile.h>
    
    ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
    ```

  * 系统调用`sendfile()`在代表输入文件的描述符`in_fd`和代表输出文件的描述符`out_fd`之间传送文件内容（字节）。描述符`out_fd`**必须指向一个套接字**，而`in_fd`指向的文件必须是可以`mmap`的。这些局限限制了sendfile的使用，使**sendfile只能将数据从文件传递到套接字上**，反之则不行。使用sendfile不仅减少了数据拷贝的次数(1次)，还减少了上下文切换(一次调用，两次切换)，数据传送始终只发生在kernel space

    <img src="imgs/os/sendfile.png" alt="sendfile" style="zoom:80%;" />

  * 在调用sendfile时，如果有其它进程截断了文件会发生什么呢？假设我们没有设置任何信号处理程序，**sendfile调用仅仅返回它在被中断之前已经传输的字节数，errno会被置为success**。如果我们在调用sendfile之前给文件加了锁，sendfile的行为仍然和之前相同，我们还会收RT_SIGNAL_LEASE的信号

  * sendfile仍然存在一次拷贝，就是页缓存到socket缓存的拷贝。现在我们仅仅需要把**缓冲区描述符传到socket缓冲区，再把数据长度传过去，这样DMA控制器直接将页缓存中的数据打包发送到网络中就可以了**

    <img src="imgs/os/zero_sendfile.png" alt="zero_sendfile" style="zoom:80%;" />

    * sendfile系统调用利用DMA引擎将文件内容拷贝到内核缓冲区去，然后将带有文件位置和长度信息的缓冲区描述符添加socket缓冲区去，这一步不会将内核中的数据拷贝到socket缓冲区中，**DMA引擎会将内核缓冲区的数据拷贝到协议引擎中去，避免了最后一次拷贝**

    * 不过这一种收集拷贝功能是需要硬件以及驱动程序支持的。**需要网卡支持 SG-DMA**（*The Scatter-Gather Direct Memory Access*）技术（和普通的 DMA 有所不同）

    * 可以在 Linux 系统通过下面这个命令，查看网卡是否支持 scatter-gather 特性：

      ```shell
      $ ethtool -k eth0 | grep scatter-gather
      scatter-gather: on
      ```

* 使用splice

  * sendfile只适用于将数据**从文件拷贝到套接字**上，限定了它的使用范围。Linux在2.6.17版本引入splice系统调用，用于**在两个文件描述符中移动数据**

  * 原型

    ```c++
      #define _GNU_SOURCE         /* See feature_test_macros(7) */
      #include <fcntl.h>
      ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags);
    ```

      * splice调用在**两个文件描述符之间移动数据**，而**不需要数据在内核空间和用户空间来回拷贝**。他从`fd_in`拷贝`len`长度的数据到`fd_out`，**但是有一方必须是管道设备，这也是目前splice的一些局限性**。如果`fd_in`是一个管道文件描述符，那么off_in必须设为NULL；如果不是，那么表示从输入数据流的何处开始读取数据，此时若`fd_in`为NULL，则表示从输入数据流的当前偏移位置读入；不为NULL，则指出具体的偏移位置。``fd_out/off_out`同理使用与于输出流。flags参数有以下几种取值
      * SPLICE_F_MOVE ：尝试去移动数据而不是拷贝数据。这仅仅是对内核的一个小提示：**如果内核不能从pipe移动数据或者pipe的缓存不是一个整页面，仍然需要拷贝数据**。Linux最初的实现有些问题，**所以从2.6.21开始这个选项不起作用**，后面的Linux版本应该会实现
      * SPLICE_F_NONBLOCK ：splice 操作不会被阻塞。然而，如果文件描述符没有被设置为不可被阻塞方式的 I/O ，那么调用 splice 有可能仍然被阻塞
      * SPLICE_F_MORE： 后面的splice调用会有更多的数据。

  * splice调用利用了Linux提出的**管道缓冲区机制**， 所以**至少一个描述符要为管道**。以上几种零拷贝技术都是**减少数据在用户空间和内核空间拷贝技术实现的**，**但是有些时候，数据必须在用户空间和内核空间之间拷贝**。这时候，我们只能**针对数据在用户空间和内核空间拷贝的时机上下功夫了**。**Linux通常利用写时复制(copy on write)来减少系统开销，这个技术又时常称作COW**

* 使用tee

  * tee函数用于**两个管道之间复制数据**，也是零拷贝操作，它不消耗数据，源文件描述符上面的数据仍然可以用于后续读操作

  * 原型

    ```c++
    #include <fcntl.h>
    ssize_t tee(int fd_in, int fd_out, size_t len, unsigned int flags);
    ```

  * 上述参数与splice相同，但fd_in和fd_out都必须是管道文件描述符。成功返回两个文件描述符之间复制的数据，返回0表示没任何数据复制，失败返回-1并设置errno

# Futex设计与实现

* futex (fast userspace mutex) 是Linux的一个基础组件，可以用来构建各种更高级别的同步机制，比如锁或者信号量等等，POSIX信号量就是基于futex构建的。大多数时候编写应用程序并不需要直接使用futex，一般用基于它所实现的系统库就够了

* 背景

  * 传统的SystemV IPC(inter process communication)**进程间同步机制都是通过内核对象来实现**的，以 semaphore 为例，当进程间要同步的时候，必须通过系统调用semop(2)进入内核进行PV操作。系统调用的缺点是开销很大，需要从**user mode切换到kernel mode、保存寄存器状态、从user stack切换到kernel stack、等等，通常要消耗上百条指令**

  * 事实上，有一部分系统调用是可以避免的，因为**现实中很多同步操作进行的时候根本不存在竞争，即某个进程从持有semaphore直至释放semaphore的这段时间内，常常没有其它进程对同一semaphore有需求**

  * 在这种情况下，内核的参与本来是不必要的，可是在传统机制下，**持有semaphore必须先调用semop(2)进入内核去看看有没有人和它竞争，释放semaphore也必须调用semop(2)进入内核去看看有没有人在等待同一semaphore，这些不必要的系统调用造成了大量的性能损耗**

  * 在futex诞生之前，linux下的同步机制可以归为两类：用户态的同步机制 和 内核同步机制。 用户态的同步机制基本上就是利用原子指令实现的spinlock。最简单的实现就是使用一个整型数，0表示未上锁，1表示已上锁。trylock操作就利用原子指令尝试将0改为1

    ```c
    bool trylock(int lockval) {
        int old;
        atomic { old = lockval; lockval = 1; }  // 如：x86下的xchg指令
        return old == 0;
    }
    ```

  * 无论spinlock事先有没有被上锁，经历trylock之后，它肯定是已经上锁了。所以lock变量一定被置1。而trylock是否成功，取决于spinlock是事先就被上了锁的（old\==1），还是这次trylock上锁的(old\==0)。而使用原子指令则可以避免多个进程同时看到old==0，并且都认为是自己改它改为1的
  * spinlock的lock操作则是一个死循环，不断尝试trylock，直到成功。 对于一些很小的临界区，使用spinlock是很高效的。因为trylock失败时，可以预期持有锁的线程（进程）会很快退出临界区（释放锁）。所以死循环的忙等待很可能要比进程挂起等待更高效
  * 但是spinlock的应用场景有限，**对于大的临界区，忙等待则是件很恐怖的事情**，特别是当同步机制运用于等待某一事件时（比如服务器工作线程等待客户端发起请求）。所以很多情况下进程挂起等待是很有必要的
  * 内核提供的同步机制，诸如semaphore、mutex等，其实骨子里也是利用原子指令实现的spinlock，内核在此基础上实现了进程的睡眠与唤醒。 使用这样的锁，能很好的支持进程挂起等待。**但是最大的缺点是每次lock与unlock都是一次系统调用，即使没有锁冲突，也必须要通过系统调用进入内核之后才能识别**
  * 理想的同步机制应该是**在没有锁冲突的情况下在用户态利用原子指令就解决问题**，而**需要挂起等待时再使用内核提供的系统调用进行睡眠与唤醒**。**换句话说，用户态的spinlock在trylock失败时，能不能让进程挂起，并且由持有锁的线程在unlock时将其唤醒**？

* futex设计思想

  * futex的解决思路是：**在无竞争的情况下操作完全在user space进行，不需要系统调用，仅在发生竞争的时候进入内核去完成相应的处理(wait 或者 wake up)**
  * 所以说，futex是一种**user mode和kernel mode混合的同步机制**，需要两种模式合作才能完成，**futex变量必须位于user space，而不是内核对象**，futex的代码也分为user mode和kernel mode两部分，**无竞争的情况下在user mode，发生竞争时则通过sys_futex系统调用进入kernel mode进行处理**

* 实现：

  ```c
  // 在uaddr指向的这个锁变量上挂起等待（仅当*uaddr==val时）
  int futex_wait(int *uaddr, int val);
  // 唤醒n个在uaddr指向的锁变量上挂起等待的进程
  int futex_wake(int *uaddr, int n);
  ```

  * 内核会动态维护一个跟uaddr指向的锁变量相关的等待队列
  * 注意futex_wait的第二个参数，由于**用户态trylock与调用futex_wait之间存在一个窗口，其间lockval可能发生变化**（比如正好有人unlock了）。所以用户态应该将自己看到的*uaddr的值作为第二个参数传递进去，futex_wait真正将进程挂起之前一定得检查lockval是否发生了变化，并且检查过程跟进程挂起的过程得放在同一个临界区中。如果futex_wait发现lockval发生了变化，则会立即返回，由用户态继续trylock
  * futex实现了锁粒度的等待队列，而这个锁却并不需要事先向内核申明。任何时候，用户态调用futex_wait传入一个uaddr，内核就会维护起与之配对的等待队列
  * 这件事情听上去好像很复杂，实际上却很简单。其实它并不需要为每一个uaddr单独维护一个队列，futex只维护一个总的队列就行了，所有挂起的进程都放在里面。当然，队列中的节点需要能标识出相应进程在等待的是哪一个uaddr。这样，当用户态调用futex_wake时，只需要遍历这个等待队列，把带有相同uaddr的节点所对应的进程唤醒就行了
  * 作为优化，futex维护的这个等待队列由若干个带spinlock的链表构成。调用futex_wait挂起的进程，通过其uaddr hash到某一个具体的链表上去。这样一方面能分散对等待队列的竞争、另一方面减小单个队列的长度，便于futex_wake时的查找。每个链表各自持有一把spinlock，将"*uaddr和val的比较操作"与"把进程加入队列的操作"保护在一个临界区中
  * 另一个问题是关于uaddr参数的比较。futex支持多进程，需要考虑同一个物理内存单元在不同进程中的虚拟地址不同的问题。那么不同进程传递进来的uaddr如何判断它们是否相等，就不是简单数值比较的事情。相同的uaddr不一定代表同一个内存，反之亦然
  * 两个进程（线程）要想共享同存，无外乎两种方式：**通过文件映射**（映射真实的文件或内存文件、ipc shmem，以及有亲缘关系的进程通过带MAP_SHARED标记的匿名映射共享内存）、**通过匿名内存映射**（比如多线程），这也是进程使用内存的唯二方式
  * 那么futex就应该支持这两种方式下的uaddr比较。匿名映射下，需要比较uaddr所在的地址空间（mm）和uaddr的值本身；文件映射下，需要比较uaddr所在的文件inode和uaddr在该inode中的偏移。注意，上面提到的内存共享方式中，有一种比较特殊：有亲缘关系的进程通过带MAP_SHARED标记的匿名映射共享内存。这种情况下表面上看使用的是匿名映射，但是内核在暗中却会转成到/dev/zero这个特殊文件的文件映射。若非如此，各个进程的地址空间不同，匿名映射下的uaddr永远不可能被futex认为相等

* 互斥锁pthread_mutex_t的实现原理

  ```c
  // pthread_mutex_lock:
  atomic_dec(pthread_mutex_t.value);
  if (pthread_mutex_t.value!=0)
    futex(WAIT)
  else
    success
  
  // pthread_mutex_unlock:
  atomic_inc(pthread_mutex_t.value);
  if(pthread_mutex_t.value!=1)
  	futex(WAKEUP)
  else
  	success
  ```

* 信号量sem_t的实现原理

  ```c
  sem_wait(sem_t *sem)
  {
  	for (;;) {
          if (atomic_decrement_if_positive(sem->count))
             break;
          futex_wait(&sem->count, 0)
      }
  }
  
  sem_post(sem_t *sem)
  {
     n = atomic_increment(sem->count);
     // Pass the new value of sem->count
     futex_wake(&sem->count, n + 1);
  }
  ```

  

