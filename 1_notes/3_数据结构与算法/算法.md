# 基本排序算法

## 总结

| 排序算法 |      平均时间复杂度       |         最好情况          |         最坏情况          |       空间复杂度       | 排序方式  | 稳定性 |
| :------: | :-----------------------: | :-----------------------: | :-----------------------: | :--------------------: | :-------: | :----: |
| 冒泡排序 |    $ \textbf{O}(n^2) $    |     $ \textbf{O}(n) $     |    $ \textbf{O}(n^2) $    |   $ \textbf{O}(1) $    | In-place  |  稳定  |
| 选择排序 |    $ \textbf{O}(n^2) $    |    $ \textbf{O}(n^2) $    |    $ \textbf{O}(n^2) $    |   $ \textbf{O}(1) $    | In-place  | 不稳定 |
| 插入排序 |   $ \textbf{O}(n^{2}) $   |     $ \textbf{O}(n) $     |    $ \textbf{O}(n^2) $    |   $ \textbf{O}(1) $    | In-place  |  稳定  |
| 希尔排序 |  $ \textbf{O}(n\log n) $  |  $ \textbf{O}(n^{1.3}) $  |    $ \textbf{O}(n^2) $    |   $ \textbf{O}(1) $    | In-place  | 不稳定 |
| 归并排序 |  $ \textbf{O}(n\log n) $  |  $ \textbf{O}(n\log n) $  |  $ \textbf{O}(n\log n) $  |   $ \textbf{O}(n) $    | Out-place |  稳定  |
| 快速排序 |  $ \textbf{O}(n\log n) $  |  $ \textbf{O}(n\log n) $  |    $ \textbf{O}(n^2) $    | $ \textbf{O}(\log n) $ | In-place  | 不稳定 |
|  堆排序  |  $ \textbf{O}(n\log n) $  |  $ \textbf{O}(n\log n) $  |  $ \textbf{O}(n\log n) $  |   $ \textbf{O}(1) $    | In-place  | 不稳定 |
|          |                           |                           |                           |                        |           |        |
| 计数排序 |    $ \textbf{O}(n+k) $    |    $ \textbf{O}(n+k) $    |    $ \textbf{O}(n+k) $    |  $ \textbf{O}(n+k) $   | Out-place |  稳定  |
|  桶排序  |    $ \textbf{O}(n+k) $    |     $ \textbf{O}(n) $     |    $ \textbf{O}(n^2) $    |  $ \textbf{O}(n+k) $   | Out-place |  稳定  |
| 基数排序 | $ \textbf{O}(n\times k) $ | $ \textbf{O}(n\times k) $ | $ \textbf{O}(n\times k) $ |  $ \textbf{O}(n+k) $   | Out-place |  稳定  |

* 平均时间复杂度同样是$ \textbf{O}(n\log n) $，为什么快速排序要比堆排序性能好？
  * 堆排序访问数据的方式没有快速排序友好
    * **对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是跳着访问的。**比如，堆排序中，最重要的一个操作就是数据的堆化。比如下面这个例子，对堆顶进行堆化，会依次访问数组下标是1，2，4，8的元素，而不像快速排序那样，局部顺序访问，所以，**这样对CPU缓存是不友好的**
  * 对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序
    * 对于基于比较的排序算法来说，整个排序过程是由两个基本操作组成的，比较和交换。**快速排序交换的次数不会比逆序的多。但是堆排序的第一步是建堆，建堆的过程会打乱数据原有的相对选择顺序，导致数据有序度降低。比如对于一组已经有序的数据来说，经过建堆之后，数据反而变得更无序了**

## 冒泡排序

```c++
// 基本冒泡排序
void bubbleSort(int array[], int length)
{
	for (int i = 0; i < length - 1; i++)
	{
		for (int j = 0; j < length - 1 - i; j++)
		{
			if (array[j] > array[j + 1])
			{
				swap(array[j], array[j + 1]);
			}
		}
	}
}

// 改进版冒泡排序
void improvedBubbleSort(int array[], int length)
{
	bool flag = true;
	for (int i = 0; i < length - 1 && flag == true; i++)
	{
		flag = false;
		for (int j = 0; j < length - 1 - i; j++)
		{
			if (array[j] > array[j + 1])
			{
				flag = true;
				swap(array[j], array[j + 1]);
			}
		}
	}
}
```

## 选择排序

```C++
// 选择排序
void selectionSort(int array[], int length)
{
	for (int i = 0; i < length - 1; i++) {
		int minIdx = i;
		for (int j = i + 1; j < length; j++) {
			if (array[j] < array[minIdx]) {
				minIdx = j;
			}
		}
		if (minIdx != i) {
			swap(array[i], array[minIdx]);
		}
	}
}
```

##　插入排序

* 最佳情况在 排序数组有序 时发生

```C++
// 插入排序
void insertionSort(int array[], int length)
{
	for (int i = 1; i < length; i++) {
		int j = i - 1;
		int tmp = array[i];
		while (j >= 0 && array[j] > tmp) {
			array[j + 1] = array[j];
			j--;
		}
		array[j + 1] = tmp;
	}
}

// 二分插入排序
void binaryInsertionSort(int array[], int length)
{
	for (int i = 1; i < length; i++)
	{
		int tmp = array[i];
		int low = 0, high = i - 1;
		while (low <= high)
		{
			int mid = low + (high - low) / 2;
			if (array[mid] > tmp)
				high = mid - 1;
			else
				low = mid + 1;
		}
		for (int j = i; j >= high + 2; j--)
		{
			array[j] = array[j - 1];
		}
		array[high + 1] = tmp;
	}
}
```

## 希尔排序

```c++
// 希尔排序
void shellSort(int array[], int length)
{
	std::vector<int> gaps;
	int h = 1;
	while (h < length) {
		gaps.push_back(h);
		h = 3 * h + 1;
	}
	int numGap = gaps.size()-1;
	for (; numGap >= 0; numGap--) {
		int gap = gaps[numGap];
		for (int i = gap; i < length; i += gap) {
			shellInsert(array, gap, i);
		}
	}
}
// 插入操作
void shellInsert(int array[], int gap, int idx) {
	int tmp = array[idx];
	int j = idx - gap;
	while (j >= 0 && array[j] > tmp) {
		array[j + gap] = array[j];
		j -= gap;
	}
	array[j + gap] = tmp;
}
```

## 归并排序

```c++
// 归并排序
void mergeSort(vector<int>& array, int length)
{
    vector<int>tmpSpace(length);
	//mergeSortRecursive(array, tmpSpace, 0, length - 1);	// 递归版本
	mergeSortIterative(array, tmpSpace, length);  	// 迭代版本
}

// 迭代版本，自底向上
void mergeSortIterative(vector<int>&array, vector<int> &tmpSpace, int length)
{
	int stride = 2;
	while (stride < length) {
		int start = 0;
		while (start + stride < length) {
			merge(array, tmpSpace, start, start + stride - 1);
			start += stride;
		}
		merge(array, tmpSpace, start, length-1);  // 末尾剩余元素
		stride *= 2;
	}
	merge(array, tmpSpace, 0, length - 1);
}

// 递归版本，自顶向下
void mergeSortRecursive(vector<int>&array, vector<int> &tmpSpace, int first, int last)
{
	if (first < last)
	{
		int mid = first +　(last－first) / 2;
		mergeSortRecursive(array, tmpSpace, first, mid);
		mergeSortRecursive(array, tmpSpace, mid + 1, last);
		merge(array, tmpSpace, first, last);
	}
}

void merge(vector<int>&array, vector<int> &tmpSpace, int first, int last)
{
    if(first < last)
    {
        int left = first, mid = first + (last - first) / 2, right = mid + 1, idx = 0;
        while (left<=mid||right<=last) {
            if (left <= mid && right <= last) {
                if (array[left] <= array[right])
                    tmpSpace[idx++] = array[left++];
                else
                    tmpSpace[idx++] = array[right++];
            }
            else if (left <= mid)
                tmpSpace[idx++] = array[left++];
            else
                tmpSpace[idx++] = array[right++];
        }
        idx = 0;
        while (first <= last)
            array[first++] = tmpSpace[idx++];
    }
}
```

## 快速排序

* 最坏情况：每次划分只能将序列分为一个元素与其他元素两部分(正序，逆序，元素全部相等)，这时的快速排序退化为冒泡排序
* 最好情况：Partition函数每次恰好能均分序列
* 与枢轴（pivot）的选择策略有关

```c++
// 递归版本快排
// 
void quickSortRecurcive(int array[], int low, int high)
{
	if (low < high) {
		int pivotIdx = partition(array, low, high);
		quickSortRecurcive(array, low, pivotIdx-1);
		quickSortRecurcive(array, pivotIdx+1, high);
	}
}

// 迭代版本
void quickSortIterative(int array[], int length)
{
    std::stack< std::pair<int,int> > range;
    int left, right, mid;
    if(length > 1) {
        left = 0;
        right = length - 1;
    	mid = partition(array, left, right);
        if(left < mid-1)
            range.push({left, mid-1});
        if(mid+1 < right)
            range.push({mid+1, right});
    }
    while(!range.empty()) {
        auto subRange = range.top();
        range.pop();
        left = subRange.first;
        right = subRange.second;
        mid = partition(array, left, right);
        if(left < mid-1)
            range.push({left, mid-1});
        if(mid + 1 < right)
            range.push({mid+1, right});
    }
}

int partition(int array[], int low, int high)
{
	// swap(array[low], array[(low + high) / 2]);   // 选中间元素作为枢轴
    // int pivot = array[low];		// 选第一个元素作为枢轴
    
	// 三数取中（median-of-three）
    int pivot = selectPivotMedianOfThree(array, low, high);
	while (low < high) {
		while (low < high && array[high] >= pivot)
			high--;
		array[low] = array[high];
		while (low < high && array[low] <= pivot)
			low++;
		array[high] = array[low];
	}
	array[low] = pivot;
	return low;
}


// 函数作用：取待排序序列中low、mid、high三个位置上数据，选取他们中间的那个数据作为枢轴
int selectPivotMedianOfThree(int arr[],int low,int high)
{
	int mid = low + ((high - low) >> 1);// 计算数组中间的元素的下标
	//使用三数取中法选择枢轴
 
	if (arr[mid] > arr[high]) // 目标: arr[mid] <= arr[high]
	{
		swap(arr[mid], arr[high]);
	}
 
	if (arr[low] > arr[high])//目标: arr[low] <= arr[high]
	{
		swap(arr[low], arr[high]);
	}
 
	if (arr[mid] > arr[low]) // 目标: arr[low] >= arr[mid]
	{
		swap(arr[mid],arr[low]);
	}
 
	//此时，arr[mid] <= arr[low] <= arr[high]
	return arr[low];
	// low的位置上保存这三个位置中间的值
	// 分割时可以直接使用low位置的元素作为枢轴，而不用改变分割函数了
}
```

## 堆排序

```C++
void heapSort(int array[], int length)
{
	buildHeap(array, length);
    // 自底向上建堆
	for (int i = length - 1; i > 0; i--) {
		swap(array[0], array[i]);   // 将最大值移动到最后，然后调整堆
		adjustHeap(array, 0, i - 1);
	}
}

void buildHeap(int array[], int length)
{
	for (int i = length / 2; i >= 0; i--)  // 自底向上
		adjustHeap(array, i, length - 1);
}

// 下沉法(sink)
void adjustHeap(int array[], int root, int last)
{
	int larger = 2 * root + 1;  // 左子节点
	while (larger <= last)  // 至少一个孩子
	{
		if (larger < last && array[larger] < array[larger + 1])  // have two children, and the right 
			larger++;											// child is larger than the left, 
		if (array[root] < array[larger])						//"larger" points to the right child
		{
			swap(array[root], array[larger]);
			root = larger;
			larger = 2 * root + 1;   // go to next level
		}
		else
			break;   // satisfy the heap property, exit
	}
}
```

## 计数排序

```c++
// 计数排序
void countSort(int arr[], int length)
{
    int minVal = INT_MAX, maxVal = INT_MIN;
    for(int i = 0; i < length; i++) {
        if(arr[i] > maxVal)
        	maxVal = array[i];
        if(arr[i] < minVal)
            minVal = arr[i];
    }
    int bias = 0 - minVal;
    int * count = new int[maxVal - minVal + 1];
    memset(count, 0, maxVal-minVal+1);
    for(int i = 0; i < length; i++) {
        count[arr[i] + bias]++;
    }
    
    for(int i = 1; i < maxVal-minVal+1; i++)
    {
        count[i] += count[i-1];
    }
    
    int * result = new int[length];
    int idx = length-1;
    while(idx>=0) {
        count[array[idx] + bias]--;
        result[count[arr[idx] + bias] - 1] = arr[idx];
        idx--;
    }
    for(int i = 0; i<length;i++)
        arr[i] = result[i];
    
    delete count;
    delete result;
}
```

## 桶排序

```c++
void bucketSort(int array[], int length)
{
    int maxVal = INT_MIN;
    int minVal = INT_MAX;
    for(int i = 0; i < length; i++)
    {
        maxVal = max(maxVal, array[i]);
        minVal = min(minVal, array[i]);
    }
    int bucketCount = (maxVal - minVal)/lenght + 1;
    std::vector<int> *bucketArr = new std::vector<int>[bucketCount];
    for(int i = 0; i < length; i++)
    {
        int num = (array[i] - minVal) / length;
        bucketArr[num].push_back(array[i]);
    }
    for(int i = 0; i < bucketCount; i++)
    {
        std::sort(bucketArr[i].begin(), bucketArr[i].end());
    }
    int idx = 0;
    for(int i = 0; i < bucketCount; i++)
    {
        for(int j = 0; j < bucketArr[i].size(); j++)
            array[idx++] = bucketArr[i][j];
    }
    delete [] bucketArr;
}
```

# 哈希

* 概念

  * 散列的概念属于查找，它**不以关键字的比较为基本操作**，采用直接寻址技术。在理想情况下，查找的期望时间为O(1)
  * hash函数就是把任意长的输入字符串变化成固定长的输出字符串的一种函数。输出字符串的长度称为hash函数的位数
  * 散列（Hashing）通过散列函数将要检索的项与索引（散列，散列值）关联起来，生成一种便于搜索的数据结构（散列表）

* 常用哈希构造函数

  * 直接定址法
    * 取关键字或关键字的某个线性函数值为哈希地址：H(key) = key 或 H(key) = a·key + b，其中a和b为常数，这种哈希函数叫做自身函数
    * 注意：由于直接定址所得地址集合和关键字集合的大小相同。因此，**对于不同的关键字不会发生冲突**。但实际中能使用这种哈希函数的情况很少
  * 相乘取整法
    * 首先用关键字key乘上某个常数A(0 < A < 1)，并抽取出key*A的小数部分；然后用m乘以该小数后取整
    * 注意：该方法最大的优点是m的选取比除余法要求更低。比如，完全可选择它是2的整数次幂。虽然该方法对任何A的值都适用，但对某些值效果会更好。Knuth建议选取 0.61803……
  * 平方取中法
    * 取关键字平方后的中间几位为哈希地址
    * 通过平方扩大差别，另外中间几位与乘数的每一位相关，由此产生的散列地址较为均匀。这是一种较常用的构造哈希函数的方法
    * 将一组关键字(0100，0110，1010，1001，0111)，平方后得(0010000，0012100，1020100，1002001，0012321)，若取表长为1000，则可取中间的三位数作为散列地址集：(100，121，201，020，123)
  * 除留余数法
    * 取关键字被数p除后所得余数为哈希地址：H(key) = key MOD p (p ≤ m)
    * 注意：这是一种最简单，也最常用的构造哈希函数的方法。它**不仅可以对关键字直接取模(MOD)，也可在折迭、平方取中等运算之后取模**。值得注意的是，在使用除留余数法时，**对p的选择很重要**。一般情况下可以**选p为质数或不包含小于20的质因素的合数**
  * 随机数法
    * 选择一个随机函数，取关键字的随机函数值为它的哈希地址，即 `H(key) = random (key)`，其中random为随机函数。通常，当关键字长度不等时采用此法构造哈希函数较恰当

* 解决哈希冲突的方法

  * 开放定址法：
    * 就是在发生冲突后，通过某种探测技术，去依次探查其他单元，直到探查到不冲突为止，将元素添加进去
    * 假如是在index的位置发生哈希冲突，那么通常有一下几种探测方式
      * 线性探测法（线性探测再散列）：向后依次探测index+1，index+2…位置，看是否冲突，直到不冲突为止，将元素添加进去。
      * 平方探测法：不探测index的后一个位置，而是探测$2^i$位置上时发生冲突，比如探测$2^0$位置上的冲突，接着探测$2^1$位置，依此类推，直至冲突解决
  * 链地址法
    * 链表法就是在发生冲突的地址处，挂一个单向链表，然后所有在该位置冲突的数据，都插入这个链表中。插入数据的方式有多种，可以从链表的尾部向头部依次插入数据，也可以从头部向尾部依次插入数据，也可以依据某种规则在链表的中间插入数据，总之保证链表中的数据的有序性。Java的HashMap类就是采取链表法的处理方案
  * 再哈希法
    * 在发生哈希冲突后，使用另外一个哈希算法产生一个新的地址，直到不发生冲突为止。这个应该很好理解
    * 再哈希法可以有效的避免堆积现象，但是缺点是不能增加了计算时间和哈希算法的数量，而且不能保证在哈希表未满的情况下，总能找到不冲突的地址
  * 建立一个公共溢出区
    * 建立一个基本表，**基本表的大小等于哈希表的大小**。**建立一个溢出表，所有哈希地址的第一个记录都存在基本表中，所有发生冲突的数据，不管哈希算法得到的地址是什么，都放入溢出表中**
    * 但是有一个缺点就是，必须事**先知道哈希表的可能大小，而且溢出表里的数据不能太多，否则影响溢出表的查询效率**。实际上就是要尽量减少冲突

* MD5加密算法

  * MD5是一个安全的散列算法，**输入两个不同的明文不会得到相同的输出值，根据输出值，不能得到原始的明文，即其过程不可逆**；所以要解密MD5没有现成的算法，只能用**穷举法**，把可能出现的明文，用MD5算法散列之后，**把得到的散列值和原始的数据形成一个一对一的映射表**，通过比在表中比破解密码的MD5算法散列值，通过匹配从映射表中找出破解密码所对应的原始明文

* 一致性哈希

  * [详见1](https://blog.csdn.net/cywosp/article/details/23397179/)

  * [详见2](https://www.jianshu.com/p/49e3fbf41b9b)

  * 概念

    * 一致性Hash是一种特殊的Hash算法，由于其均衡性、持久性的映射特点，被广泛的应用于**负载均衡领域和分布式存储**，如nginx和memcached都采用了一致性Hash来作为集群负载均衡的方案
    * 普通的Hash函数最大的作用是散列，或者说是将一系列在形式上具有相似性质的数据，打散成随机的、均匀分布的数据。**不难发现，这样的Hash只要集群的数量N发生变化，之前的所有Hash映射就会全部失效**。如果集群中的每个机器提供的服务没有差别，倒不会产生什么影响，但对于分布式缓存这样的系统而言，**映射全部失效就意味着之前的缓存全部失效，后果将会是灾难性的**。一致性Hash通过构建**环状的Hash空间代替线性Hash空间**的方法解决了这个问题

  * 良好的分布式cahce系统中的一致性hash算法应该满足以下几个方面

    * **平衡性(Balance)**：平衡性是指哈希的结果能够**尽可能分布到所有的缓冲中去**，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件
    * **单调性(Monotonicity)**：单调性是指如果**已经有一些内容通过哈希分派到了相应的缓冲中**，又有**新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区**
    * **分散性(Spread)**：在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。**当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中**。这种情况显然是应该避免的，**因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率**。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性
    * **负载(Load)**：负载问题实际上是从另一个角度看待分散性问题。**既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容**。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量**降低缓冲的负荷**
    * **平滑性(Smoothness)**：平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的

  * **原理**

    * 一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数$H$的值空间为$0 \sim 2^{32}-1$（即哈希值是一个32位无符号整形），整个哈希空间环如下：整个空间按顺时针方向组织，0和$2^{32}-1$在零点中方向重合

      <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/dhs.png" alt="dhs" style="zoom:80%;" />

    * 下一步**将各个服务器使用Hash进行一次哈希**，具体可以选择服务器的**ip或主机名作为关键字**进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用ip地址哈希后在环空间的位置如下

      <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/dhs_align.jpg" alt="dhs_align" style="zoom:50%;" />

      接下来使用如下算法定位数据访问到相应服务器：**将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器**

    * 例如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下。根据一致性哈希算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上

      <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/dhs_align_.jpg" alt="dhs_align_" style="zoom:50%;" />

  * **一致性哈希算法的容错性和可扩展性**

    * 现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，**只有C对象被重定位到Node D**。一般的，在一致性哈希算法中，**如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中后一台服务器（即沿着顺时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响**

    * 如果在系统中增加一台服务器Node X，如下图所示

      <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/dhs_align_2.jpg" alt="dhs_align_2" style="zoom:50%;" />

      此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X 。一般的，在一致性哈希算法中，**如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响**

# 红黑树

* 定义及性质

  * 一般的，红黑树，满足以下性质，即只有满足以下全部性质的树，我们才称之为红黑树：
    * 每个结点要么是红的，要么是黑的
    * 根结点是黑的
    * 每个叶结点（叶结点即指树尾端NIL指针或NULL结点）是黑的
    * 如果一个结点是红的，那么它的俩个儿子都是黑的
    * 对于任一结点而言，其到叶结点树尾端NIL指针的每一条路径都包含相同数目的黑结点

* **红黑树的各种操作的时间复杂度都是$\textbf{O}(\log n)$**

* **红黑树相比于BST和AVL树有什么优点？**

  * 红黑树是**牺牲了严格的高度平衡的优越条件**为代价，它只要求部分地达到平衡要求，降低了对旋转的要求，从而提高了性能。红黑树能够以$\textbf{O}(\log n)$的时间复杂度进行搜索、插入、删除操作。此外，**由于它的设计，任何不平衡都会在三次旋转之内解决**。当然，还有一些更好的，但实现起来更复杂的数据结构能够做到一步旋转之内达到平衡，但红黑树能够给我们一个比较“便宜”的解决方案
  * 相比于BST，因为**红黑树可以能确保树的最长路径不大于两倍的最短路径的长度**，所以可以看出它的查找效果是有最低保证的。在最坏的情况下也可以保证$\textbf{O}(\log n)$的，这是要好于二叉查找树的。因为二叉查找树最坏情况可以让查找达到$\textbf{O}(n)$
  * 红黑树的算法时间复杂度和AVL相同，但统计性能比AVL树更高，所以在插入和删除中所做的后期维护操作肯定会比红黑树要耗时好多，但是他们的查找效率都是$\textbf{O}(\log n)$，所以红黑树应用还是高于AVL树的。**实际上插入 AVL 树和红黑树的速度取决于你所插入的数据。如果你的数据分布较好，则比较宜于采用 AVL树(例如随机产生系列数)，但是如果你想处理比较杂乱的情况，则红黑树是比较快的**

* **红黑树相对于哈希表，在选择使用的时候有什么依据？**

  * 权衡三个因素: 查找速度，数据量，内存使用，可扩展性
  * 总体来说，hash查找速度会比map快，而且查找速度基本和数据量大小无关，属于常数级别；而map的查找速度是log(n)级别。并不一定常数就比log(n) 小，hash还有hash函数的耗时。如果考虑效率，特别是在元素达到一定数量级时，考虑使用hash。但若你对内存使用特别严格， 希望程序尽可能少消耗内存，那么一定要小心，hash可能会让你陷入尴尬，特别是当你的hash对象特别多时，你就更无法控制了，而且 hash的构造速度较慢

  * 红黑树并不适应所有应用树的领域。如果数据基本上是静态的，那么让他们待在他们能够插入，并且不影响平衡的地方会具有更好的性能。如果数据完全是静态的，例如，做一个哈希表，性能可能会更好一些
  * 红黑树天生有序，而对哈希表排序比较麻烦，涉及范围查找，或者排序，红黑树更优

  * 在实际的系统中，例如，需要使用动态规则的防火墙系统，使用红黑树而不是散列表被实践证明具有更好的伸缩性。Linux内核在管理vm_area_struct时就是采用了红黑树来维护内存块的

  * 红黑树通过扩展节点域可以在不改变时间复杂度的情况下得到结点的秩

# [跳表](https://zhuanlan.zhihu.com/p/54869087)

* https://www.jianshu.com/p/9d8296562806

# LRU

* LRU是Least Recently Used的缩写，即最近最少使用，是一种常用的页面置换算法，**选择最近最久未使用的页面予以淘汰**。该算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间 t，当须淘汰一个页面时，选择现有页面中其 t 值最大的，即最近最少使用的页面予以淘汰

  > 设计LRU缓存结构，该结构在构造时确定大小，假设大小为K，并有如下两个功能
  >
  > - set(key, value)：将记录(key, value)插入该结构
  > - get(key)：返回key对应的value值
  >
  > [要求]
  >
  > 1. set和get方法的时间复杂度为O(1)
  > 2. 某个key的set或get操作一旦发生，认为这个key的记录成了最常使用的。
  > 3. 当缓存的大小超过K时，移除最不经常使用的记录，即set或get最久远的。
  >
  > 若opt=1，接下来两个整数x, y，表示set(x, y)
  > 若opt=2，接下来一个整数x，表示get(x)，若x未出现过或已被移除，则返回-1
  > 对于每个操作2，输出一个答案

* 代码实现，使用哈希表和链表实现，哈希表记录某个key值对应的链表节点，以常数时间复杂度操作对应链表节点。每次set或get，将对应节点移动到链表头，表示最近使用，每次淘汰将链表尾节点淘汰，表示最久未使用

  ```c++
  class LRUCache {
  public:
      LRUCache(int capacity): cap(capacity) {
  
      }
      
      int get(int key) {
          if(map.count(key)) {
              mv2front(key);
              return map[key]->second;
          }
          return -1;
      }
      
      void put(int key, int value) {
          if(map.count(key)) {
              map[key]->second = value;
              mv2front(key);
          }
          else{
              cache.push_front({key, value});
              map.insert({key, cache.begin()});
              if(map.size() > cap) {
                  map.erase(cache.back().first);
                  cache.pop_back();
              }
              
          }
      }
      
  private:
      void mv2front(int key) {
          cache.splice(cache.begin(), cache, map[key]);
      }
      
  private:
      int cap;
      unordered_map<int, list<pair<int, int> >::iterator> map;
      list<pair<int, int> > cache;
  };
  ```

# LFU

* LFU（least frequently used (LFU) page-replacement algorithm），即最不经常使用页置换算法，要求在页置换时置换引用计数最小的页，因为经常使用的页应该有一个较大的引用次数

  > 一个缓存结构需要实现如下功能。
  >
  > - set(key, value)：将记录(key, value)插入该结构
  > - get(key)：返回key对应的value值
  >
  > 但是缓存结构中最多放K条记录，如果新的第K+1条记录要加入，就需要根据策略删掉一条记录，然后才能把新记录加入。这个策略为：在缓存结构的K条记录中，哪一个key从进入缓存结构的时刻开始，被调用set或者get的次数最少，就删掉这个key的记录；
  >
  > 如果调用次数最少的key有多个，上次调用发生最早的key被删除
  >
  > 这就是LFU缓存替换算法。实现这个结构，K作为参数给出
  >
  > [要求]
  >
  > set和get方法的时间复杂度为O(1)
  >
  > 若opt=1，接下来两个整数x, y，表示set(x, y)
  > 若opt=2，接下来一个整数x，表示get(x)，若x未出现过或已被移除，则返回-1

* 代码实现，使用两个哈希表实现。将每个访问频率的记录存储到一个链表中，每个节点记录为（key，val），一个哈希表建立访问频率到对应链表首节点的映射，另一个哈希表建立记录链表中记录的key值到该节点的映射，实现常数时间复杂度访问。每次get或set，将该记录访问频率加1，并添加到对应访问频率链表头。每次淘汰最小访问频率对应链表的尾节点

  ```c++
  
  class LFUImpl {
  public:
      vector<int> LFU(vector<vector<int> >& operators, int k) {
          vector<int>　ret;
          for(auto &op: operators) {
              if(op[0] == 1){
                  set(op[1], op[2], k);
              }
              else {
                  ret.push_back(get(op[1]));
              }
          }
          return ret;
      }
      
      int get(int key ){
          if(key2nodeit.count(key)) {
              update(key);
              return key2nodeit[key]->val;
          }
          return -1;
      }
      
      void set(int key, int val, int k) {
          if(key2nodeit.count(key)>0){
              update(key);
              key2nodeit[key]->val = val;
          }
          else {
              if(int(key2nodeit.size()) == k){
                  remove();
              }
              minFreq = 1;
              freq2node[minFreq].push_front({key, val, minFreq});
              key2nodeit[key] = freq2node[minFreq].begin();
          }
      }
  private:
      void update(int key) {
          int val = key2nodeit[key]->val;
          int freq = key2nodeit[key]->freq;
          freq2node[freq].erase(key2nodeit[key]);
          if(freq2node[freq].empty()) {
              freq2node.erase(freq);
              if(minFreq == freq) {
                  minFreq++;
              }
          }
          freq2node[freq + 1].push_front(node(key, val, freq + 1));
          key2nodeit[key] = freq2node[freq + 1].begin();
      }
   
      void remove() {
          int key = freq2node[minFreq].back().key;
          key2nodeit.erase(key);
          freq2node[minFreq].pop_back();
          if(freq2node[minFreq].empty()) {
              freq2node.erase(minFreq);
          }
      }
  private:
      struct node{
          int key, val, freq;
          node(int k, int v, int f) : key(k), val(v), freq(f){}
      };
      unordered_map<int, list<node>::iterator > key2nodeit;	// 每个键对应一个list迭代器，用于快速查询
      unordered_map<int, list<node> > freq2node; // 每个频率对应一个list
      int minFreq = 1;  // 记录最小频率
  };
  ```

# KMP算法

* 代码实现

  ```c++
  class Solution {
  public:
      int strStr(string target, string pattern) 
      {
          if(pattern.empty())
              return 0;
          if(target.size() < pattern.size())
              return -1;
          vector<int> next(pattern.size(), 0);
          findNext(pattern, next);
          int i = 0, j = 0, tlen = target.size(), plen = pattern.size();
          while(i < tlen && j < plen) {
              if(j == -1 || target[i] == pattern[j]) {
                  i++;
                  j++;
              }
              else {
                  j = next[j];
              }    
          }
          if(j >= plen) {
              return i - plen;
          }
          return -1;
      }
  
      void findNext(string & str, vector<int> &next)
      {
          next[0] = -1;
          int i = 0, k = -1;
          int len = str.size();
          while(i < len - 1) {
              if(k == -1 || str[i] == str[k]) {
                  i++;
                  k++;
                  if(str[i] != str[k]) {
                   	next[i] = k;   
                  }
                  else {
                      next[i] = next[k];
                  }
              }
              else {
                  k = next[k];
              }  
          }
      }
  };
  ```
  

# 取模(mod)基本运算法则

1. $(a+b)\% p=(a\% p+b\% p)\% p$
2. $(a-b)\% p=(a\% p-b\% p)\% p$
3. $(a\times b)\% p=(a\% p\times b\% p)\% p$
4. $a^b\% p=((a\% p)^b)\% p$
5. 由1得：$(\sum_{1}^{n}x)\%p=(\sum_{1}^{n}x\%p)\%p$
6. 结合律
   1. $((a+b)\%p+c)\%p=(a+(b+c)\%p)\%p$
   2. $((a\times b)\%p \times c)\%p= (a \times (b\times c)\%p)\%p$
7. 交换律
   1. $(a+b)\%p=(b+a)\%p$
   2. $(a\times b)\%p=(b\times a)\%p$
8. 分配律
   1. $(a+b)\%p=(a\%p+b\%p)\%p$
   2. $((a+b)\%p\times c)\%p = ( (a\times c)\%p + (b\times c)\%p )\%p$

# 大数越界情况下的求余问题

* 解决方案：循环求余和快速幂求余法，其中后者的时间复杂度更低，两种方法均基于以下求余运算规则推出：
  $$
  (xy)\odot p=[(x\odot p)(y\odot p)]\odot p
  $$

## 循环求余法

* 根据求余运算性质推出
  $$
  x^a \odot p=[(x^{a−1}\odot p)(x\odot p)]\odot p=[(x^{a−1}\odot p)x]\odot p,其中x<p,x\odot p=x
  $$

* 解析： 利用此公式，可通过循环操作依次求$ x^1, x^2, ..., x^{a-1}, x^a $对 $p$ 的余数，保证每轮中间值 `rem` 都在 `int32` 取值范围中，代码如下

  ```c++
  int remainder(int x, int a, int p){
      int rem = 1;
      while(a--){
          rem = (rem * x) % p;
      }
      return rem;
  }
  ```

* 时间复杂度 $O(N)$ ： 即循环的线性复杂度

## 快速幂求余

* 根据求余运算性质推出
  $$
  x^a \odot p=(x^2)^{a/2}\odot p
  $$

* 当a为奇数时，a/2不是整数，分如下两种情况
  $$
  x^a \odot p=\left\{\begin{align}
    &(x^2\odot p)^{a//2}\odot p&,		a为偶数\\
    &[(x\odot p)(x^{a-1}\odot p)]\odot p=[x(x^2\odot p)^{a//2}]\odot p&,a为奇数
  
  \end{align}\right.
  $$
  
* 解析： 利用以上公式，可通过循环操作每次把指数 a 问题降低至指数 a//2问题，只需循环 $log_2(N)$ 次，因此可将复杂度降低至对数级别，实现代码如下

  ```c++
  int fast_remainder(int x, int a, int p){
      int rem = 1;
      while(a>0){
          if(a & 1 == 0){
              rem = (rem * x) % p;
          }
          x = x*x % p;
          a = a / 2;
      }
      return rem;
  }
  ```

# 图的基本概念和术语
1. 图的定义
$$\begin{aligned}
&G=(V,E) \\
&V\text{(Vertex):顶点（数据元素）的有穷非空集合；} \\
&E\text{(Edge):边的有穷集合。}
\end{aligned}$$
2. 有向图：图中每条边都是有方向的，有向图的边也称作“弧”
3. 无向图：图中每条边都是无方向的
4. 完全图：图中任意两个顶点之间都有一条边相连。完全图分为有向完全图和无向完全图，如下图示
<img src="https://img-blog.csdnimg.cn/20201023102435499.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =500x" alt="在这里插入图片描述" style="zoom:67%;" />
若图中有$n$个顶点，无向完全图共有$C_n^2=\frac{n(n-1)}{2}$条边，而有向完全图共有$2C_n^2=n(n-1)$条边。
5. 稀疏图：有很少边或弧的图（$e<n\log n$）,$e$为边或弧的数目，$n$为顶点数目
6. 稠密图：有较多边或弧的图
7. 网：边/弧带权的图
8. 邻接：有边/弧相连的两个顶点之间的关系。在无向图中，存在$(V_i,V_j)$，则称$V_i$和$V_j$**互为邻接点**；同样，在有向图中，存在$<V_i,V_j>$，则称$V_i$**邻接到**$V_j$，$V_j$**邻接于**$V_i$。
9. 关联（依附）：边/弧与顶点之间的关系。存在$(V_i,V_j)$/$<V_i,V_j>$，则称该边关联于$V_i$和$V_j$。
10. 顶点的度：与某顶点$v$相关联的边/弧的数目，记为TD($v$)。在有向图中，顶点的度等于该顶点的**入度**和**出度**之和。顶点$v$的入度是以$v$为终点的有向边的条数，记作ID($v$)；而顶点$v$的出度是以$v$为始点的有向边的条数，记作OD($v$)。
11. 路径：接续的边构成的顶点序列。
12. 路径长度：路径上的边/弧数目，或权值之和。
13. 回路（环）：第一个顶点和最后一个顶点相同的路径。
14. 简单路径：除路径起点和终点可以相同外，其他顶点均不相同的路径。
15. 简单回路（简单环）：除路径起点和终点相同外，其余顶点均不相同的路径。
<img src="https://img-blog.csdnimg.cn/20201023233353684.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =500x" alt="在这里插入图片描述" style="zoom: 50%;" />
16. 连通图/强连通图：在无/有向图$G=(V,\{E\})$中，若对任意两个顶点$v,u$都存在从$v$到$u$的路径，则称$G$是连通图/强连通图。
<img src="https://img-blog.csdnimg.cn/20201023234146290.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =500x" alt="在这里插入图片描述" style="zoom: 50%;" />
17. 权与网：图中边或弧所具有的相关系数称为权。**带权的图称为网**。
18. 子图：设有两个图$G=(V,\{E\})$，$G_1=(V_1,\{E_1\})$，若$V_1\subseteq V$，$E_1\subseteq E$，则称$G_1$是$G$的子图。
<img src="https://img-blog.csdnimg.cn/20201023234754289.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =500x" alt="在这里插入图片描述" style="zoom:50%;" />
19. 极大连通子图：若某子图$G_1$是$G$的连通子图，若$G$的任一不在$G_1$的顶点加入$G_1$，得到的子图不再连通，则称$G_1$为$G$的极大连通子图。
20. 连通分量：无向图$G$的极大连通子图称为$G$的连通分量。
<img src="https://img-blog.csdnimg.cn/20201023235549327.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =500x" alt="在这里插入图片描述" style="zoom:50%;" />
21. 极大强连通子图：若某子图$G_1$是$G$的强连通子图，若$G$的任一不在$G_1$的顶点加入$G_1$，得到的子图不再强连通，则称$G_1$为$G$的极大连通子图。
22. 强连通分量：有向图$G$的极大强连通子图称为$G$的强连通分量。
<img src="https://img-blog.csdnimg.cn/20201024000013317.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =500x" alt="在这里插入图片描述" style="zoom:50%;" />
23. 极小连通子图：若某子图$G_1$是$G$的连通子图，若删除$G_1$的任一条边，得到的子图不再连通，则称$G_1$为$G$的极小连通子图。
24. 生成树：包含无向图$G$所有顶点的极小连通子图。
25. 生成森林：对非连通图，由各个连通分量的生成树组成的集合。
<img src="https://img-blog.csdnimg.cn/20201024000619100.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =500x" alt="在这里插入图片描述" style="zoom:50%;" />
# 图的存储结构
1. 数组表示法：邻接矩阵
2. 链表存储结构：邻接表、邻接多重表、十字链表
## 邻接矩阵表示法
表示方法：建立一个**顶点表**（一维数组），记录顶点信息，再建立一个**邻接矩阵**（二维数组）表示各个顶点之间的关系。设图$G=(V,E)$​有$n$​个顶点，则顶点表 Vtxs[$n$​ ]为


|    $i$     |  $0$  |  $1$  |  $2$  |   ...   | $n-1$ |
| :--------: | :---: | :---: | :---: | :-----: | :---: |
| Vtxs[$i$ ] | $v_1$ | $v_2$ | $v_3$ | **...** | $v_n$ |

而图的邻接矩阵arcs\[$n$​][$n$]定义为：
$$
\text{arcs}[i][j]= 
	\begin{cases}1,\quad  & 
	  \text{如果}(v_i,v_j)/<v_i,v_j>\in E,\\ 
	  0, \quad  & \text{否则}.
	\end{cases}
$$

### 无向图的邻接矩阵表示法
<img src="https://img-blog.csdnimg.cn/20201025112835482.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =550x" alt="在这里插入图片描述" style="zoom:50%;" />
	特点1：无向图的邻接矩阵是对称的。
	特点2：顶点$v_i$的度是第$i$行（列）中1的个数。
	特别地，完全图的邻接矩阵对角线元素为0，其余为1。

### 有向图的邻接矩阵表示法
<img src="https://img-blog.csdnimg.cn/20201025115318630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =550x" alt="在这里插入图片描述" style="zoom:50%;" />
在有向图的邻接矩阵中，
第$i$行的含义：以顶点$v_i$为尾的弧（即出度边）；
第$i$列的含义：以顶点$v_i$为头的弧（即入度边）。
特点1：有向图的邻接矩阵可能是不对称的。
特点2：顶点$v_i$的出度是第$i$行元素之和；顶点$v_i$的入度是第$i$列元素之和；所以顶点$v_i$的度=第$i$行元素之和 + 第$i$列元素之和。

### 网（即有权图）的邻接矩阵表示法
$$
	\text{arcs}[i][j]= 
	\begin{cases}w_{ij},\quad  & 
	  \text{如果}(v_i,v_j)/<v_i,v_j>\in E,\\ 
	  \infty, \quad  & \text{无边弧}.
	\end{cases} 
$$
<img src="https://img-blog.csdnimg.cn/20201025124422848.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =550x" alt="在这里插入图片描述" style="zoom:50%;" />

### 邻接矩阵表示法优缺点
优点：
* 直观、简单、好理解
* 方便检查任意一对顶点之间是否存在边
* 方便找任一顶点的所有邻接点（有边直接相连的顶点）
* 方便求得任一顶点的度

缺点：
* 不利于增加和删除节点
* 浪费空间，存稀疏图时有大量无效元素
* 统计稀疏图总边数时浪费时间

## 邻接表表示法（链表）
邻接表表示方法：
* **顶点**存储：按编号顺序将顶点数据存储在一维数组中
* **关联同一顶点的边**（以顶点为尾的弧）的存储：用线性链表存储
<img src="https://img-blog.csdnimg.cn/20201026204620875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =550x" alt="在这里插入图片描述" style="zoom:50%;" />
### 无向图邻接表表示法
<img src="https://img-blog.csdnimg.cn/20201026205100440.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =550x" alt="在这里插入图片描述" style="zoom:50%;" />
特点：

* 邻接表**不唯一**
* 若无向图有$n$个节点，$e$条边，则其邻接表需要$n$个头节点和$2e$个表节点。适合存储稀疏图。
* 无向图中节点$v_i$的**度**为第$i$个存储节点关联边的**单链表的节点数**。
* 计算节点的度容易
### 有向图邻接表表示法
有向图邻接表的表示方法与无向图的表示方法类似，但只将以对应节点为尾的弧存储在单链表中。如下图示
<img src="https://img-blog.csdnimg.cn/20201026211415152.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =400x" alt="在这里插入图片描述" style="zoom:50%;" />
特点： 

* 若无向图有$n$个节点，$e$条边，则其邻接表需要$n$个头节点和$e$个表节点。
* 顶点$v_i$的**出度**为第$i$个单链表的节点数。
* 顶点$v_i$的**入度**为全部单链表中邻接点域值为$i-1$的节点的个数
* 找出度容易，找入度难

有向图邻接表的表示方法还可以用逆邻接表法，即单链表存储的是以对应节点为头的弧。
## 邻接矩阵和邻接表的关系
* 联系
	* 邻接表中每个链表对应邻接矩阵中的一行，链表中节点个数等于邻接矩阵一行中非零元素的个数。
* 区别
	* 对于任一确定的无向图，其邻接矩阵是唯一的（行列号和顶点编号一致）， 但邻接表不唯一（链接次序与顶点编号无关）。
	* 邻接矩阵空间复杂度为$\Omicron(n^2)$，邻接表空间复杂度为$\Omicron(n+e)$。
	* 用途：邻接矩阵多用于稠密图，邻接表多用于稀疏图。
## 十字链表
* 十字链表（Orthogonal List）是解决有向图的邻接表存储方式中求节点的度困难的问题，它是有向图的另一种链式存储结构，可以看成是有向图的邻接表和逆邻接表的结合。
* 有向图中每一条弧对应十字链表中一个**弧节点**，同时有向图中每一个顶点在十字链表中对应有一个**顶点节点**。
<img src="https://img-blog.csdnimg.cn/20201029222043364.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =500x" alt="在这里插入图片描述" style="zoom:50%;" />
## 邻接多重表
* 邻接多重表解决无向图邻接表存储方式中，每条边要存储两次的问题。
<img src="https://img-blog.csdnimg.cn/20201030131227788.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =500x" alt="在这里插入图片描述" style="zoom:50%;" />
# 图的遍历
## 定义
* 从给定的连通图中某一顶点除法，沿着一些边访遍图中所有顶点，且每个顶点仅被访问一次，就叫做图的遍历，它是图的**基本操作**。
* 遍历的实质：寻找每个顶点的邻接点的过程。
## 遍历可能存在的问题及解决思路
* 图中可能存在回路，且图的任一顶点都可能与其他顶点相通，访问完某个顶点后可能沿着某些边又回到曾经访问过的顶点。
* 解决思路：设置辅助数组visited[*n*]，用来标记被访问过的顶点
	* 初始状态，辅助数组visited中所有值为0。
	* 顶点*i*被访问，则将visited[*i*]设置为1，防止被多次访问。
## 深度优先搜索（Depth First Search, DFS）
遍历方法：
* 在访问图中某一起始点$v$后，由$v$出发，访问它的任一邻接点$w_1$；
* 再从$w_1$出发，访问与$w_1$邻接但还没被访问过的顶点$w_2$；
* 然后再从$w_2$出发，重复类似的访问过程，......，如此进行下去，直到所有邻接顶点都被访问过的顶点$u$为止。
* 接着，退回一步，退到前一次刚访问过的顶点，看其是否由未被访问过的邻接点：
	* 如果有，则访问此顶点，再由此顶点出发，进行与先前类似的访问过程。
	* 如果没有，则再回退一步进行搜索。重复上述过程，直到连通图中所有顶点都被访问过为止。
	<img src="https://img-blog.csdnimg.cn/20201031233559889.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =500x" alt="在这里插入图片描述" style="zoom:50%;" />

* 连通图的深度优先遍历的思想类似于树中先序遍历。
* 算法时间复杂度
	* 用邻接矩阵表示的图，遍历图中每一顶点都要完整扫描每一顶点所在的行，时间复杂度为$O(n^2)$。
	* 用邻接表表示的图，虽然有$2e$个表节点，但依靠辅助数组，只需扫描$e$个数组即可完成遍历，再加上访问$n$个头节点的时间，时间复杂度为$O(n+e)$。
* 空间复杂度：借用堆栈，$O(n)$。
## 广度优先搜索（Breadth First Search, BFS）
遍历方法：
* 从图的某一顶点$v_i$出发，首先依次访问该顶点的所有邻接点$v_{i_1},v_{i_2},...,v_{i_n}$，再按这些邻接点的被访问的先后次序访问与它们邻接的所有未被访问的顶点。
* 重复此过程，直到所有顶点都被访问为止。
<img src="https://img-blog.csdnimg.cn/20201031233721570.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =500x" alt="在这里插入图片描述" style="zoom: 50%;" />
* 连通图的广度优先遍历的思想类似于树中的层次遍历。
* 算法时间复杂度：同DFS。
* 空间复杂度：借用队列，$O(n)$。
# 生成树
## 生成树的概念及特点
* 生成树：所有顶点都由边连在一起，但**不存在回路**的图。
* 一个图可以有许多棵不同的生成树。
<img src="https://img-blog.csdnimg.cn/20201104212850431.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =400x" alt="在这里插入图片描述" style="zoom:50%;" />
* 所有生成树均有以下特点：
	* 生成树的节点个数与图的顶点个数相同。
	* 生成树树的**极小连通子图**，去掉任一条边则不再连通。
	* **一个n个顶点的连通图的生成树有n-1条边**。
	* 在生成树中**再加一条边必形成回路**。
	* 生成树中任意两点间的路径**唯一**。
* 含n个顶点n-1条边的图不一定是生成树。
## 无向图的生成树的构建
设图$G=(V,E)$是一个连通图，当从图的任一顶点出发遍历$G$时，将边集$E(G)$分成两个集合$T(G)$和$B(G)$。其中，$T(G)$是遍历是经过的边的集合，$B(G)$是未经过的边的集合。显然$G_1=(V,T)$是$G$的极小连通子图，即$G_1$是$G$的生成树。
<img src="https://img-blog.csdnimg.cn/20201104222039726.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =500x" alt="在这里插入图片描述" style="zoom:50%;" />

## 最小生成树
定义：给定一个无向网络，在该网的所有生成树中，使得各边**所有权值之和最小**的生成树称为该网的最小生成树（Minimum Spanning Tree，MST），也叫最小代价生成树。
<img src="https://img-blog.csdnimg.cn/20201104222554750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =500x" alt="在这里插入图片描述" style="zoom:50%;" />

### 最小生成树的性质
* 构造最小生成树的算法很多利用MST的性质。
* 设$N=(V,E)$是一个连通网，$U$是顶点集$V$的一个非空子集。设$u\in U,v\in V-U$若边$(u,v)$是一条具有最小权值的边，则必存在一棵包含$(u,v)$的最小生成树。
* 性质解释：在生成树的构造过程中，图中$n$个顶点分属两个不同的集合：
	* 已落在生成树上的顶点集：$U$
	* 尚未落在生成树上的顶点集：$V-U$
* 接下来应该在所有连通$U$中顶点和$V-U$中顶点的边中选取**权值最小的**边。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201105222830540.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =400x)
### 构造最小生成树——Prim算法
* 算法思想
	* 设$N=(V,E)$是一个连通网，$TE$是$N$上最小生成树中**边的集合**。
	* 初始令$U=\{u_0\}$，$u_0\in V$，$U=\{\}$。
	* 在所有$u\in U$，$v\in V-U$构成的边$(u，v)\in E$中，找出一条**代价（权值）最小**的边$(u_0,v_0)$。
	* 将$(u_0,v_0)$加入$TE$，并同时将$v_0$加入$U$。
	* 重复上两步骤，直至$U=V$，此时$T=(U,TE)$为$N$的最小生成树。
### 构造最小生成树——Kruskal算法
* 算法思想
	* 设$N=(V,E)$是一个连通网，令最小生成树为只有$n$个顶点而无边的非连通图$T=(V,\{\})$，且**每个顶点自成一个连通分量**。
	* 在$E$中选取代价最小的边，若该边依附的顶点落在$T$中不同的连通分量上（即**不能形成环**），则将此边加入$T$中；否则，舍弃此边，选取下一条代价最小的边。
	* 依次类推，直至$T$中所有顶点都在同一连通分量上为止。
### 两种算法比较
| 算法       | Prim算法               | Kruskal算法              |
| ---------- | ---------------------- | ------------------------ |
| 算法思想   | 选择点                 | 选择边                   |
| 时间复杂度 | $O(n^2)$ ($n$为顶点数) | $O(e\log e)$ ($e$为边数) |
| 适应范围   | 稠密图                 | 稀疏图                   |
# 最短路径问题

* 问题定义：在**有向网**中，A点（源点）到达B点（重点）的所有路径中，寻找一条**各边权值之和最小**的路径，即是最短路径问题。
* 特点：最短路径与最小生成树不同，路径上不一定包含$n$个顶点，也不一定包含$n-1$条边。
* **两类问题**
	* 第一类问题：两点间最短路径，使用**Dijkstra算法**。
	* 第二类问题：某源点到其他各点的最短路径，使用**Floyd算法**。
## Dijkstra最短路径算法
* 单源最短路径算法。
* 算法步骤：
	1. 初始化：初始时，令已检测的点集为$S=\varnothing$，图中其余未检测的点集为$T=\{v_i\},i=0...n-1$。定义一个距离辅助数组$D[n]$记录源点到其他各点的最短路径值，$D[1...n-1]$值为$\infty$，$D[0]=0$。
	2. 选择：$T$中选择出令$D[j]$最小的顶点$v_j$加入$S$，并将其从$T$中移去。
	3. 更新：遍历$v_j$在$T$中的所有邻接顶点：
		若在$T$中存在$v_j$的邻接点$v_k$，使得$weight(<v_j,v_k>)+D[j] < D[k]$，则用$weight(<v_j,v_k>)+D[j]$ 的值更新$D[k]$。
	4. 重复步骤2，3，直至$T=\varnothing, S=V$。
	5. 结束时，$D[i]$即为源点到$v_i$的最短路径值。

* 上述算法**只能计算到源点到各个顶点的最短路径距离，但不能得到该最短路径**。若要获得最短路径，则需要定义一个记录前驱顶点的辅助数组$predecessor$。在满足更新条件的时候，还要将被更新的顶点的前驱记录在数据中。例如，满足更新条件$weight(<v_j,v_k>)+D[j] < D[k]$的同时，记录被更新的$v_k$的前驱：$predecessor[k] = predecessor[j]$。取某个顶点的最短路径时，只需沿着该顶点往前寻找前驱，直到到达源点。
## Floyd最短路径算法
* 计算所有顶点间的最短路径，多源最短路径算法。
* 算法思想：逐个点试探，从$v_i$到$v_j$的所有可能存在的路径中，选出一条长度最短的路径。
* 算法步骤
	* 初始时设置一个$n$阶方阵，令其对角元素为0，若存在弧$<v_i,v_j>$，则方阵中对应$[i,j]$元素为权值，否则为$\infty$。
	* 逐步在原直接路径中增加中间节点，若加入中间节点后路径变短，则修改之；否则维持原值。所有顶点试探完毕，算法结束。
# 有向无环图及其应用
## 基本概念
* 有向无环图：无环的有向图，简称DAG（Directed Acycline Graph）图。
<img src="https://img-blog.csdnimg.cn/20201114231448938.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =400x" alt="在这里插入图片描述" style="zoom:50%;" />
* 有向无环图通常用来描述一个工程或者系统的进行过程。（通常把计划、施工、生产、程序流程等当成是一个工程。）
* 一个工程可以分成若干个子工程，只要完成了这些子工程，就可以导致整个工程的完成。
## AOV网
* 用一个有向图表示一个工程的各子工程及其相互制约的关系。其中以顶点表示活动，弧表示活动之间的优先制约关系，称这种有向图为顶点表示活动的网，简称AOV网（Activity On Vertex network）。
* 特点：
	* 若从$i$到$j$有一条有向路径，则$i$是$j$的前驱，$j$是$i$的后继。
	* 若$<i,j>$是网中有向边，则$i$是$j$的直接前驱，$j$是$i$的直接后继。
	* AOV网中，不允许存在回路，因为如果有回路存在，则表明某项活动以自身为先决条件，显然这是荒谬的。
* 问题：如何判断AOV网中是否存在回路？
### 拓扑排序
* 在AOV网没有回路的前提下，将全部活动排列称一个线性序列，使得若AOV网中有弧$<i,j>$存在，则在这个序列中，$i$一定排在$j$的前面，具有这种性质的线性序列称为**拓扑有序序列**，相应的拓扑有序排序的算法称为**拓扑排序**。
* 方法步骤：
	* 在有向图中选一个没有前驱的顶点且输出之。
	* 在图中删除该顶点和所有以它为尾的弧。
	* 重复上面两步，直至全部顶点均已输出；或者当图中不存在无前驱的顶点为止。
	<img src="https://img-blog.csdnimg.cn/20201116203936837.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =450x" alt="在这里插入图片描述" style="zoom:50%;" />
* 检测AOV网中是否存在环的方法：对有向图构造其顶点的拓扑有序序列，若网中所有顶点都在它的拓扑有序序列中，则该AOV网必定不存在环。

## AOE网
* 用一个有向图表示一个工程的各子工程及其相互制约的关系。其中以弧表示活动，顶点表示活动的开始或结束时间，称这种有向图为边表示活动的网，简称AOE网（Activity On Edge network）。
* 解决关键路径问题。
* 把工程计划表示为边表示活动的网络，用顶点表示时间，弧表示活动，弧的权值表示活动持续时间。
* 事件表示在它之前的活动已经结束，在它之后的活动可以开始。
<img src="https://img-blog.csdnimg.cn/20201117213901874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjY1Nzk2,size_16,color_FFFFFF,t_70#pic_center =600x" alt="在这里插入图片描述" style="zoom:50%;" />
