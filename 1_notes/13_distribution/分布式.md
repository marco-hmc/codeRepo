# 集群

## **负载均衡**

* 集群中的应用服务器（节点）通常被设计成无状态，用户可以请求任何一个节点

* 负载均衡器会根据集群中每个节点的负载情况，将用户请求转发到合适的节点上

* 负载均衡器可以用来实现高可用以及伸缩性：

  * 高可用：当某个节点故障时，负载均衡器会将用户请求转发到另外的节点上，从而保证所有服务持续可用；
  * 伸缩性：根据系统整体负载情况，可以很容易地添加或移除节点

* 负载均衡器运行过程包含两个部分：

  * 根据负载均衡算法得到转发的节点
  * 进行转发

* 负载均衡算法

  * **轮询（Round Robin）**

    * 轮询算法把每个请求轮流发送到每个服务器上

    * 下图中，一共有 6 个客户端产生了 6 个请求，这 6 个请求按 (1, 2, 3, 4, 5, 6) 的顺序发送。(1, 3, 5) 的请求会被发送到服务器 1，(2, 4, 6) 的请求会被发送到服务器 2。该算法比较适合每个服务器的性能差不多的场景，如果有**性能存在差异**的情况下，那么**性能较差的服务器可能无法承担过大的负载**（下图的 Server 2）

      <img src="imgs/distribution/rr.png" alt="rr" style="zoom:80%;" />
      
      ```java
      // ServerIps.java
      public class ServerIps{
          public static final List<String> SERVER_LIST = Arrays.asList(
          	"192.168.0.1",
              "192.168.0.1",
              "192.168.0.2",
              "192.168.0.3",
              "192.168.0.4",
              "192.168.0.5",
          );
      }
      
      // RoundRobinServer.java
      public class RoundRobinServer{
          private static Integer pos = 0;
          public String getServer(){
              Integer n = ServerIps.SERVER_LIST.size();
              if(pos == n){
                  pos = 0;
              }
              String ip = ServerIps.SERVER_LIST.get(pos);
              ++pos;
              return ip;
      }
      ```
  
  * **加权轮询（Weighted Round Robbin）**

    * 加权轮询是在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值，**性能高的服务器分配更高的权值**
  
    * 例如下图中，服务器 1 被赋予的权值为 5，服务器 2 被赋予的权值为 1，那么 (1, 2, 3, 4, 5) 请求会被发送到服务器 1，(6) 请求会被发送到服务器 2
  
      <img src="imgs/distribution/wrr.png" alt="wrr" style="zoom:80%;" />
      
      ```java
      // WeightedServerIps.java
      public class WeightedServerIps{
          public static final Map<String, Integer> WEIGHTED_SERVER_LIST = new LinkedHashMap<String, Integer>();
          // 权重之和为15
          WEIGHTED_SERVER_LIST.put("192.168.0.1", 1);
          WEIGHTED_SERVER_LIST.put("192.168.0.2", 2);
          WEIGHTED_SERVER_LIST.put("192.168.0.3", 3);
          WEIGHTED_SERVER_LIST.put("192.168.0.4", 4);
          WEIGHTED_SERVER_LIST.put("192.168.0.5", 5);
      }
      
      // RequestId.java  模拟产生请求号
      public class RequestId{
          private Integer id = 0;
          public static Integer getAndIncrement(){
              return num++;
          }
      }
      
      // WeightedRoundRobinServer.java   缺陷：可能会导致大量连续的请求涌入同一台权值较大的服务器
      public class WeightedRoundRobinServer{
          public static String getServer(){
              Integer totalWeight = 0;
              for(Integer weight : WeightedServerIps.WEIGHTED_SERVER_LIST.values()){
              	totalWeight += weight;
          	}
              
          	// 请求号取余
          	Integer pos = RequestId.getAndIncrement() % totalWeight;
              
              for(String ip : WeightedServerIps.WEIGHTED_SERVER_LIST.keySet()){
                  if(pos < WeightedServerIps.WEIGHTED_SERVER_LIST.get(ip)){
                      return ip;
                  }
                  pos -= WeightedServerIps.WEIGHTED_SERVER_LIST.get(ip);
              }
              return "";
          }
      }
      ```
  
  * **平滑加权轮询法（Smooth Weight Round Robin）**
  
    * 由nginx作者提出，重要！
  
    * 思路
  
      * 假设有 $N$ 台服务器 $S = \{s_0, s_1, s_2, …, s_n\}$，默认权重为 $W = \{w_0, w_1, w_2, …, w_n\}$，当前权重为 $CW = \{cw_0, cw_1, cw_2, …, cw_n\}$
  
      * 在该算法中有两个权重，**默认权重表示服务器的原始权重**，**当前权重表示每次访问后重新计算的权重**，当前权重的初始值为默认权重值，**当前权重值最大的服务器为** $maxWeightServer$，**所有默认权重之和**为 $weightSum$，服务器列表为$serverList$，算法可以描述为：
  
        1. 找出当前权重值最大的服务器 $maxWeightServer$；
        2. 计算$ \{w_0, w_1, w_2, …, w_n\}$ 之和 $weightSum$；
        3. 将 $maxWeightServer.cw = maxWeightServer.cw - weightSum$，得到的当前权重记为$CWN$
        4. 重新计算$\{s_0, s_1, s_2, …, s_n\}$​ 的当前权重 $CW$​，计算公式为 $CW=W+CWN$​，作为下一轮次的$CW$​
        5. 返回 $maxWeightServer$
  
      * 假如有3台服务器$S = \{s_0, s_1, s_2\}$，默认权重分别为$W=\{1, 2, 3\}$，当前权重的初始值为默认权重$CW = \{3,4,5\}$，权重之和$weightSum=6$，其运行流程如下表
  
        | 轮次 |      CW      | maxWeight | weightSum | maxWeightServer（返回） | $CWN(-weigthSum)$ |
        | :--: | :----------: | :-------: | :-------: | :---------------------: | :---------------: |
        |  1   | $\{1,2,3\}$  |    $3$    |    $6$    |          $s_2$          |   $\{1,2,-3\}$    |
        |  2   | $\{2,4,0\}$​  |    $4$    |    $6$    |          $s_1$          |   $\{2,-2,0\}$​    |
        |  3   | $\{3,0,3\}$​​  |    $3$​    |    $6$    |          $s_0$          |   $\{-3,0,3\}$​    |
        |  4   | $\{-2,2,6\}$​ |    $6$​    |    $6$    |          $s_2$​          |   $\{-2,2,0\}$​    |
        |  5   | $\{-1,4,3\}$​ |    $4$​    |    $6$    |          $s_1$          |   $\{-1,-2,3\}$​   |
        | ...  |     ...      |    ...    |    ...    |           ...           |        ...        |
  
  * **最少连接（least Connections）**
  
    * 由于**每个请求的连接时间不一样**，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成负载不均衡
  
    * 例如下图中，(1, 3, 5) 请求会被发送到服务器 1，但是 (1, 3) 很快就断开连接，此时只有 (5) 请求连接服务器 1；(2, 4, 6) 请求被发送到服务器 2，只有 (2) 的连接断开，此时 (6, 4) 请求连接服务器 2。该系统继续运行时，服务器 2 会承担过大的负载
  
      <img src="imgs/distribution/lc.png" alt="lc" style="zoom:80%;" />
  
    * 最少连接算法就是**将请求发送给当前最少连接数的服务器**上。例如下图中，服务器 1 当a前连接数最小，那么新到来的请求 6 就会被发送到服务器 1 上
  
      <img src="imgs/distribution/lc2.png" alt="lc2" style="zoom:80%;" />
  
  * **加权最少连接（Weighted Least Connection）**
  
    * 在最少连接的基础上，**根据服务器的性能为每台服务器分配权重**，再**根据权重计算出每台服务器能处理的连接数**
  
  * **随机算法（Random）**
  
    * 把请求随机发送到服务器上
    * 和轮询算法类似，该算法比较适合**服务器性能差不多**的场景
  
    ```java
    // 简单随机算法实现
    // ServerIps.java
    public class ServerIps{
        public static final List<String> SERVER_LIST = Arrays.asList(
        	"192.168.0.1",
            "192.168.0.1",
            "192.168.0.2",
            "192.168.0.3",
            "192.168.0.4",
            "192.168.0.5",
        );
    }
    
    // RandomServer.java
    public class RandomServer{
        public static String getServer(){
            var random = new java.util.Random();
            int rand = random.nextInt(ServerIps.SERVER_LIST.size());
            return ServerIps.SERVER_LIST.get(rand);
        }
    }
    ```
  
    ```java
    // 加权随机算法实现
    // WeightedServerIps.java
    public class WeightedServerIps{
        public static final Map<String, Integer> WEIGHTED_SERVER_LIST = new LinkedHashMap<String, Integer>();
        // 权重之和为15
        WEIGHTED_SERVER_LIST.put("192.168.0.1", 1);
        WEIGHTED_SERVER_LIST.put("192.168.0.2", 2);
        WEIGHTED_SERVER_LIST.put("192.168.0.3", 3);
        WEIGHTED_SERVER_LIST.put("192.168.0.4", 4);
        WEIGHTED_SERVER_LIST.put("192.168.0.5", 5);
    }
    
    // WeightedRandomServer.java
    public class WeightedRandomServer{
        public static String getServer(){
            List<String> ips = new ArrayList<>();
            for(String ip : WeightedServerIps.WEIGHTED_SERVER_LIST.keySet()){
                Integer weight = WeightedServerIps.WEIGHTED_SERVER_LIST.get(ip);
                // 按权重进行复制，空间利用率低，时间复杂度也会增大
                for(int i = 0; i < weight; ++i){
                    ips.add(ip);
                }
            }
            var random = new java.util.Random();
            int randomPos = random.nextInt(ips.size());
            return ips.get(randomPos);
        }
    }
    
    // WeightedRandomServerV2.java，改进版本
    public class WeightedRandomServerV2{
        public static String getServer(){
            Integer totalWeight = 0;
            for(Integer weight : WeightedServerIps.WEIGHTED_SERVER_LIST.values()){
                totalWeight += weight;
            }
            var random = new java.util.Random();
            int randomWeight = random.nextInt(totalWeight);
            for(String ip : WeightedServerIps.WEIGHTED_SERVER_LIST.keySet()){
                Integer curWeight = WeightedServerIps.WEIGHTED_SERVER_LIST.get(ip);
                if(randomWeight < curWeight){
                    return ip;
                }
                randomWeight -= curWeight;
            }
            return "";
        }
    }
    ```
  
  * **源地址哈希法 (IP Hash)**
  
    * **源地址哈希通过对客户端 IP 计算哈希值之后，再对服务器数量取模得到目标服务器的序号**
    * 可以保证同一 IP 的客户端的请求会转发到同一台服务器上，**用来实现会话粘滞**（Sticky Session）

## [**负载均衡转发实现**](http://mp.weixin.qq.com/s?__biz=MzAxODI5ODMwOA==&mid=2666552178&idx=3&sn=7e7738f8f8f88baa04f2ccd5949e7bbf&chksm=80dc9fd9b7ab16cf5b8851da03964cbe2057b28e695a010c2a96fd36fab783fd2cdc85fb668e&mpshare=1&scene=24&srcid=0316FpUVYnUunhN6Cw2AFmeZ&sharer_sharetime=1615831853059&sharer_shareid=0722ed5128948b6436f8552f291f9b0b#rd)

* **HTTP 重定向**

  * HTTP 重定向负载均衡服务器使用某种负载均衡算法计算得到服务器的 IP 地址之后，将该地址写入 HTTP 重定向报文中，**状态码为 302**

  * 客户端收到重定向报文之后，需要重新向服务器发起请求

  * 缺点：

    * **需要两次请求，因此访问延迟比较高**HTTP **负载均衡器处理能力有限**，会**限制集群的规模**该负载均衡转发的缺点比较明显，实际场景中很少使用它

      <img src="imgs/distribution/http_redirect.png" alt="http_redirect" style="zoom:80%;" />

* **DNS 域名解析**

  * 在 DNS 解析域名的同时使用负载均衡算法计算服务器 IP 地址

  * 优点：

    * DNS 能够根据地理位置进行域名解析，**返回离用户最近的服务器 IP 地址**

  * 缺点：

    * 由于 DNS 具有**多级结构**，**每一级的域名记录都可能被缓存**，当**下线一台服务器需要修改 DNS 记录时，需要过很长一段时间才能生效**

    * 大型网站基本使用了 DNS 做为**第一级负载均衡手段**，然后在**内部使用其它方式做第二级负载均衡**。也就是说，**域名解析的结果为内部的负载均衡服务器 IP 地址**

      <img src="imgs/distribution/dns_parse.png" alt="dns_parse" style="zoom:80%;" />

* **反向代理服务器**

  * 作用于应用层的模式，也被称作为**七层负载均衡**。比如常见的Nginx，性能一般可以达到万级。这种方式部署简单，成本低，而且容易扩展
  * 反向代理服务器位于源服务器前面，**用户的请求需要先经过反向代理服务器才能到达源服务器**。反向代理可以用来进行**缓存、日志记录**等，同时也可以用来做为负载均衡服务器
  * 在这种**负载均衡转发方式下，客户端不直接请求源服务器，因此源服务器不需要外部 IP 地址，而反向代理需要配置内部和外部两套 IP 地址**
  * 优点：
    * 与其它功能集成在一起，部署简单
  * 缺点：
  * **所有请求和响应都需要经过反向代理服务器，它可能会成为性能瓶颈**

* **网络层**

  * 在操作系统内核进程获取网络数据包，根据负载均衡算法计算源服务器的 IP 地址，并修改请求数据包的目的 IP 地址，最后进行转发源服务器返回的响应也需要经过负载均衡服务器，通常是让负载均衡服务器同时作为集群的网关服务器来实现
  * 常见的有LVS（Linux Virtual Server），通常性能可以支持10万级并发
  * 优点：
    * 在内核进程中进行处理，性能比较高
  * 缺点：
    * **和反向代理一样，所有的请求和响应都经过负载均衡服务器，会成为性能瓶颈链路层在**

* **链路层**

  * 根据负载均衡算法计算源服务器的 MAC 地址，并修改请求数据包的目的 MAC 地址，并进行转发
  * 通过配置源服务器的虚拟 IP 地址和负载均衡服务器的 IP 地址一致，从而不需要修改 IP 地址就可以进行转发。也正因为 IP 地址一样，所以源服务器的响应不需要转发回负载均衡服务器，可以直接转发给客户端，避免了负载均衡服务器的成为瓶颈
  * 这是一种**三角传输模式**，被称为**直接路由**。对于提供下载和视频服务的网站来说，直接路由避免了大量的网络传输数据经过负载均衡服务器
  * 这是目前大型网站使用最广负载均衡转发方式，在 Linux 平台可以使用的**负载均衡服务器为 LVS**（Linux Virtual Server）
  
* [CDN](https://www.zhihu.com/question/36514327?rf=37353035)

## 集群下Session管理

一个用户的 Session 信息如果存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器，由于服务器没有用户的 Session 信息，那么该用户就需要重新进行登录等操作

* **Sticky Session**

  * 需要**配置负载均衡器，使得一个用户的所有请求都路由到同一个服务器，这样就可以把用户的 Session 存放在该服务器中**

  * 缺点：当服务器宕机时，将丢失该服务器上的所有 Session

    <img src="imgs/distribution/sticky_session.png" alt="sticky_session" style="zoom:80%;" />

* **Session Replication**

  * 在服务器之间进行 **Session 同步操作**，**每个服务器都有所有用户的 Session 信息**，因此用户可以向任何一个服务器进行请求

  * **缺点**：

    * 占用过多内存
    * 同步过程占用网络带宽以及服务器处理器时间

    <img src="imgs/distribution/session_replication.png" alt="session_replication" style="zoom:80%;" />

* **Session Server**

  * **使用一个单独的服务器存储 Session 数据**，可以使用传统的 MySQL，也可以使用 Redis 或者 Memcached 这种内存型数据库
  * 优点：
    * 实现session共享
    * 可以水平扩展(增加redis服务器)
    * 服务器重启session不丢失(不过要注意session在redis中的刷新、失效机制)
    * 不仅可以跨服务器共享session，甚至可以跨平台(例如网页端和app端)、跨语言(java，go，python等)

  * 缺点：需要去实现存取 Session 的代码，并且多访问一次存储服务器

    <img src="imgs/distribution/server_session.png" alt="server_session" style="zoom:80%;" />

# 分布式ID

## 使用场景

* 互联网应用中，某个表可能要占用很大的物理存储空间，为了解决该问题，使用数据库分片技术。将一个数据库进行拆分，通过数据库中间件连接。如果数据库中该表选用ID自增策略，则可能产生重复的ID，此时应该使用分布式ID生成策略来生成ID
* 但随着数据日渐增长，主从同步也扛不住了，就需要对数据库进行分库分表，但分库分表后需要有一个唯一ID来标识一条数据，数据库的自增ID显然不能满足需求；特别一点的如订单、优惠券也都需要有`唯一ID`做标识。此时一个能够生成`全局唯一ID`的系统是非常必要的。那么这个`全局唯一ID`就叫`分布式ID`
* **分布式ID需要满足那些条件？**
  - 全局唯一：必须保证ID是全局性唯一的，基本要求
  - 高性能：高可用低延时，ID生成响应要块，否则反倒会成为业务瓶颈
  - 高可用：100%的可用性是骗人的，但是也要无限接近于100%的可用性
  - 好接入：要秉着拿来即用的设计原则，在系统设计和实现上要尽可能的简单
  - 趋势递增：最好趋势递增，这个要求就得看具体业务场景了，一般不严格要求

## 生成方案

* ID是数据的唯一标识，**传统的做法是利用UUID和数据库的自增ID**，在互联网企业中，大部分公司使用的都是MySql，并且因为需要事物支持，所以通常会使用Innodb存储引擎，UUID太长以及无序，所以并不合适在Innodb中作为主键，自增ID比较适合。但是随着公司业务的发展，数据量将越来越大，需要**对数据进行分表**。分表后，**每个表中的数据的ID都会按各表的节奏进行自增，很有可能出现ID冲突**。这是就需要一个单独的机制来负责生成唯一ID，生成出的ID也可以叫做**分布式ID**，或者**全局ID**。

<img src="imgs/distribution/id_methods.jpg" alt="id_methods" style="zoom:80%;" />

* https://zhuanlan.zhihu.com/p/152179727
* https://zhuanlan.zhihu.com/p/140078865

### 雪花算法

* SnowFlake算法生成id的结果是一个64bit大小的整数，它的结构如下图

  <img src="imgs/distribution/snowflake.jpg" alt="snowflake" style="zoom: 67%;" />

  * **1bit**，不用，因为二进制中最高位是符号位，1表示负数，0表示正数。生成的id一般都是用正数，所以**最高位固定为0**

  * **41bit-时间戳**，用来记录时间戳，**毫秒级**
    - 41 bit可以表示$2^{41}$个数字
    - 如果只用来表示正整数（计算机中正数包含0），可以表示的数值范围是：0 至 $2^{41}-1$，减1是因为可表示的数值范围是从0开始算的，而不是1
    - 也就是说41位可以表示$2^{41}-1$个毫秒的值，转化成单位年则是$(2^{41}-1) / (1000 \times 60 \times 60 \times 24 \times365) = 69$年

  * **10bit-工作机器id**，用来记录工作机器id
    * 可以部署在$2^{10} = 1024$个节点，包括5位数据中心机房标识datacenterId和5位单机机器标识workerId
    * 5 bit可以表示的最大正整数是$2^{5}-1 = 31$，即可以用0、1、2、3、....31这32个数字，来表示不同的datecenterId或workerId

  * **12bit-序列号**，序列号，用来记录同毫秒内产生的不同id
    * 12 bit可以表示的最大正整数是$2^{12}-1 = 4095$，即可以用0、1、2、3、....4094这4096个数字，来表示同一机器同一时间截（毫秒)内产生的4096个ID序号

* SnowFlake可以保证

  * 所有生成的ID按时间趋势递增
  * 整个分布式系统内不会产生重复ID，因为有datacenterId和workerId做区分
  
* 优点：

  * 每个毫秒值包含的id值很多，不够可以变动位数来增加，性能佳(依赖workid实现)
  * 时间戳值在高位，中间是固定的机器码，自增的序列在低位，整个id是趋势递增的
  * 能够根据业务场景数据库节点布置灵活调整bit划分，灵活度高

* 缺点
  * 强依赖于机器时钟，如果发生时钟回拨，会导致重复的id生成，所以一般基于此的算法发现始终回拨，都会抛异常处理，阻止id生成，这可能导致服务不可用

# CAP理论——强一致性

* 数据一致性(consistency)：如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据，对调用者而言，数据具有强一致性
* 服务可用性(availability)：所有**读写请求在一定时间内得到响应**，可终止，不会一直等待
* 分区容错性(partition-tolerance)：在网络分区的情况下，被分割的节点仍能对外正常服务

单体架构CA可以完美存在，但分布式架构则相反。

如果选择了CA而放弃了P，**那么当发生分区现象时，为了保证C，系统需要禁止写入，当有写入请求时，系统返回error**(例如，当前系统不允许写入)，**这又和A冲突了，因为A要求返回no error和no timeout**。**因此，分布式系统理论上不能选择CA架构，只能选择CP或者AP架构**

反证：

​		如果CAP三者可同时满足，由于允许P的存在，则一定存在节点之间的丢包，如此则不能保证C。因为允许分区容错，写操作可能在节点1上成功，在节点2失败。这时对于client1(读取节点1)和client 2(读取节点2)，就会读取到不一致的情况。如果要保持一致性，写操作必须同时失败，也就是降低系统的可用性

# Base理论——最终一致性

* Base理论是CAP理论的一种妥协，由于CAP只能二取其一，Base理论降低了发生分区容错时对可用性和一致性的要求
  1. 基本可用：允许可用性降低，可能响应延长、服务降级
  2. 软状态：指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性
  3. 最终一致性：节点数据同步可以存在时延，但在一定的期限后必须达到数据的一致，状态变为最终状态

# 数据一致性模型

* 强一致性：但更新操作完成之后，任何多个后续进程的访问都会返回最新的更新过的值，这种是对用户最友好的。就是用户上一次写什么，下一次就保证能读到什么。根据CAP理论，这种实现需要牺牲可用性
* 弱一致性：系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体地承诺多久之后可以读到。用户读到某一操作对系统数据的更新需要一段时间，我们称这段时间为“不一致性窗口”
* 最终一致性：最终一致性是弱一致性的特例，强调的是所有的数据副本，在经过一段时间的同步之后，最终都能达到一个一致的状态。因此最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。到达最终一致性的时间，就是不一致窗口时间，在没有故障发生的前提下，不一致性窗口的时间主要受通信延迟，系统负载和复制副本的个数影响
* 最终一致性模型根据其提供的不同保证可以划分为更多的模型，包括因果一致性和会话一致性等
* 因果一致性：要求有因果关系的操作顺序得到保证，非因果关系的操作顺序则无所谓
  * 进程A在更新玩某个数据项之后通知了进程B，那么进程B之后对该数据项的访问都应该能够取到进程A更新后的最新值，并且如果进程B要对该数据项进行更新操作的话，务必基于进程A更新后的最新值
  * 在微博或者微信进行评论的时候，比如你在朋友圈发了一张照片，朋友给你评论了，而你对朋友的评论进行了恢复，这条朋友圈的显示中，你的回复必须在朋友之后，这是一个因果关系，而其他没有因果关系的数据，可以允许不一致
* 会话一致性：将对系统数据的访问过程框定在了一个会话中，约定了系统能保证在同一个有效会话中实现“读己之所写”的一致性，就是在你的一次访问中，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。**实际开发中有分布式的session一致性问题，可以认为是会话一致性的一个应用**

# 选举机制Quorum、WARO

* WARO：一种简单的副本控制协议，**写操作时、只有当所有的副本都更新成功之后，这次写操作才算成功，否则视为失败**。优先保证读、任何节点读到的数据都是最新数据，牺牲了更新服务的可用性、只要有一个副本宕机，写服务就不会成功。但只要有一个节点存活，仍能提供读服务
* Quorum：10个副本，一次成功更新了三个，**那么至少需要读取八个副本的数据**，**可以保证读到了最新的数据**。**无法保证强一致性，也就是无法实现任何时刻任何用户或节点都可以读到最近一次成功提交的副本数据**。需要**配合一个获取最新成功提交的版本号的metadata服务**，这样可以确定最新已经成功提交的版本号，然后从已经读到的数据中就可以确认最新写入的数据

# paxos算法

* Paxos、Raft等分布式一致性算法则可在一致性和可用性之间取得很好的平衡，**在保证一定的可用性的同时，能够对外提供强一致性**，因此Paxos、Raft等分布式一致性算法**被广泛的用于管理副本的一致性，提供高可用性**

* paxos算法解决的是一个分布式系统如何就某个值(决议)达成一致(**分布式一致性问题**)。一个典型的场景是，在一个分布式数据库系统中，如果各个节点的初始状态一致，每个节点执行相同的操作序列，那么他们最后能够达到一个一致的状态。为了保证每个节点执行相同的操作序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。在paxos算法中，有三种角色：Proposer(提议者)，Acceptor(接受者)和Learner(记录员)

  * Proposer提议者：只要Proposer发的提案Propose被半数以上的Acceptor接受，Proposer就认为该提案的value被选定
  * Acceptor接受者：只要Acceptor接受了某个提案，Acceptor就认为该提案的value被选定了
  * Learner记录员：Acceptor告诉Learner哪个value被选定，Learner就认为哪个value被选定

* paxos算法分为两个阶段：

  * 阶段一(Prepare，请求中只包含提案编号Proposal ID)：
    * Proposer收到client请求或者发现本地有未提交的值，选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求Prepare(N)
    * Acceptor收到一个提案编号为N的Prepare请求，如果该轮paxos
      * Acceptor节点之前没有做过任何承诺(promise，预请求)，**向该Preparer节点响应promise(N)**，**承诺以后不再响应提案编号小于等于(<=)N的Prepare请求，不会通过编号小于(<)N 的提案(proposal)**（两个承诺）
      * Acceptor节点之前做出过承诺(预请求)，**对比之前承诺的提案编号N1和N**，若N1>N，则丢弃该Prepare请求，拒绝回应；否则(N1<N)，进入下一步
        * Acceptor节点之前没有接受过提案，向该Preparer节点做出承诺，响应promise(N)
        * Acceptor节点之前接受过提案，不违背以前作出的承诺下，响应已经Accept过的提案中Proposal ID最大的那个提案的Value和Proposal ID：promise(M,V)
  * 阶段二(Accept)：
    * 如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么它就会发送一个针对[N,V]提案的Accept请求(Accept(N,V))给半数以上的Acceptor。V就是收到的响应中最大提案编号对应的value，如果响应中不包含任何value，那么V就由Proposer自己决定
    * 如果Acceptor收到一个针对编号为N的提案的Accept请求，Acceptor对比已承诺的提案编号N1，如果N1小于等于N，则接受该提案，并记录N和对应的value V。否则拒绝请求，响应当前承诺的提案编号N1
  * Proposer如果收到的大多数Acceptor响应，则选定该value值，并同步给Learner，使未响应的Acceptor达成一致

  <img src="imgs/distribution/paxos.jpg" alt="paxos" style="zoom:80%;" />

* 活锁：accept时被拒绝，加大N，重新accept，此时另外一个Proposer也进行相同操作，导致accept一直失败，无法完成算法

* multi-paxos：区别于paxos只是确定一个值，multi-paxos可以确定多个值，收到accept请求后，则一定时间内不再accept其他节点的请求，以此保证后续的编号不需要再经过prepare确认，直接进行accept操作。此时该结点成为了leader，直到accept被拒绝，重新发起prepare请求竞争leader资格

* [总结](https://zhuanlan.zhihu.com/p/31780743)：
  * 获取一个Proposal ID n，为了保证Proposal ID唯一，可采用时间戳+Server ID生成；
  * Proposer向所有Acceptors广播Prepare(n)请求；
  * Acceptor比较n和minProposal，如果n>minProposal，minProposal=n，并且将 acceptedProposal 和 acceptedValue 返回；
  * Proposer接收到过半数回复后，如果发现有acceptedValue返回，将所有回复中acceptedProposal最大的acceptedValue作为本次提案的value，否则可以任意决定本次提案的value；
  * 到这里可以进入第二阶段，广播Accept (n,value) 到所有节点；
  * Acceptor比较n和minProposal，如果n>=minProposal，则acceptedProposal=minProposal=n，acceptedValue=value，本地持久化后，返回；否则，返回minProposal。
  * 提议者接收到过半数请求后，如果发现有返回值result >n，表示有更新的提议，跳转到1；否则value达成一致。

# raft算法

* raft是一个分布式一致性算法：raft会选举出leader，leader完全负责replicated log的管理。leader负责接受所有客户端更新(写)请求，然后复制到follower节点，并在“安全”的时候执行这些请求。如果leader故障，followers会重新选举出新的leader
* 三种状态：一个节点任一时刻处于下列三者身份之一
  * leader：只有一个。处理所有的客户端请求，如果客户端将请求发给了follower，follower将请求重定向给leader
  * follower：不会发送任何请求，指挥简单地响应来自leader或candidate的请求
  * candidate：作为候选人，用于选举产生新的leader

* term：任期，leader产生到重新选举为一个任期，每个节点都维持着当前的任期号
  * term是递增的，存储在log日志的entry中，代表当前entry是哪一个term时期写入
  * **每个任期只能有一个leader或者没有(选举失败)**
  * **每次rpc通信时传递该任期号，如果rpc收到任期号大于本地的、切换为follower，小于本地任期号则返回错误信息**
* 两个rpc通信
  * RequestVote rpc：负责选举，包含参数lastIndex，lastTerm
  * AppendEntries rpc：负责数据的交互
* 日志序列：每一个节点上维持着一份持久化log，通过一致性协议算法，保证每一个节点中的log保持一致，并且顺序存放，这样客户端就可以在每一个节点中读取到相同的数据
* 状态机：日志序列同步到多数节点时，leader将该日志提交到状态机，并在下一次心跳通知所有节点提交状态机(携带最后提交的lastIndex)

* 何时触发选举？
  * 集群初始化时，都是follower，随机超时(随机sleep，先唤醒的先发出选举)，变成candidate，发起选举
  * 如果follower在election timeout内没有收到来自leader的心跳，则主动触发选举
* 选举过程：发出选举的节点角度
  * 增加节点本地的term，切换到candidate状态
  * 投自己一票
    * 其他节点投票逻辑：每个节点同一任期最多只能投一票，候选人知道的信息不能比自己少(通过副本日志和安全机制保障)，先来先得
  * 并行给其他节点发送RequestVote RPCs(选举请求)、包含term参数
  * 等待回复
    * 收到majority(大多数)的投票，赢得选举，切换到leader状态，立刻给所有节点发心跳消息
    * 被告知别人当选，切换到follower状态。原来的leader对比term，比自己的大，切换到follower状态
    * 一段时间没收到majority和leader的心跳通知，则保持candidate，重新发出选举

* 日志序列同步：日志需要存储在磁盘持久化，崩溃可以从日志回复
  * 客户端发送命令给leader
  * leader把日志条目**追加**到自己的日志序列里
  * leader发送AppendEntries RPC请求给所有follower。携带了prevLogIndex，prevLogTerm，follower收到后，进行日志序列匹配 
    * 匹配上则追加到自己的日志序列
    * 匹配不上则拒绝请求，**leader将日志index调小**，重新同步直至匹配上，follower将leader的日志序列覆盖到本地
* 一旦新的日志序列条目变成majority，将日志序列应用到状态机中
  * leader在状态机里提交自己日志序列条目，然后返回结果给客户端
  * leader下次发送AppendEntries PRC时，告知follower已经提交的日志序列条目信息(lastIndex)
  * follower收到PRC后，提交到自己的状态机里
* leader提交状态机时，如果term为上一任期，必须与当前任期数据一起提交，否则可能出现覆盖已经提交状态机的日志
* 新选举出的leader一定拥有所有已提交的状态机的日志条目
  * leader在当日志序列条目已经复制到大多数follower机器上时，才会提交日志条目
  * 而选出的leader的logIndex必须大于等于大多数节点，因此leader肯定有最新的日志
* 安全原则
  * 选举安全原则：对一个给定的任期号，最多只会有一个leader被选举出来
  * 状态机安全原则：如果一个leader已经在给定的索引值位置的日志条目应用到状态机中，那么其他任何的服务器在这个索引位置不会提交一个不同的日志
  * leader完全原则：如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大的任期号的所有leader中
  * leader只附加原则：leader绝对不会删除或者覆盖自己的日志，只会追加
  * 日志匹配原则：如果两个日志在相同的索引位置的日志条目的任期号相同，那么我们就认为这个日志从头到这个索引位置之间全部完全相同

# zab协议

* zab协议是为分布式协调服务Zookeeper专门设计的一种**支持崩溃恢复的原子广播协议**，实现分布式数据一致性所有客户端的请求都是写入到Leader中，然后，由Leader同步到其他节点，称为Follower。在集群数据同步的过程中，如果出现Follower节点崩溃或者Leader进程崩溃时，都会通过zab协议来保证数据一致性
* zab协议包括两种基本模式：**崩溃恢复**和**消息广播**
* 消息广播
  * 集群中所有的事务请求都由Leader节点来处理，其他服务器为Follower，Leader将客户端的事务请求转换为事务Proposal，并且将Proposal分发给集群中的其他所有的Follower
  * 完成广播之后，Leader等待Follower反馈，当有过半数的Follower反馈信息后，Leader将再次像集群内Follower广播Commit信息，Commit信息就是确认将之前的Proposal提交
  * Leader节点的写入是一个两步操作，第一步是广播事务操作，第二步是广播提交操作，其中过半数指的是反馈的节点数>=N/2+1，N是全部Follower的节点数
* 崩溃恢复
  * 在以下情况开启新一轮Leader选举，**选举产生的Leader会与过半的Follower进行同步，使数据一致，当与过半的机器同步完成后，就退出恢复模式**，然后进入消息广播模式
    * 初始化集群，刚刚启动的时候
    * Leader崩溃，因为故障宕机
    * Leader失去了半数的机器支持，与集群中超过一般的节点断连

* 整个zk集群的一致性保证就是在上面两个状态之间切换，当Leader服务正常时，就是正常的消息广播模式；当Leader不可用时，则进入崩溃恢复模式，崩溃恢复阶段会进行数据同步，完成以后，重新进入消息广播阶段
* Zxid是zab协议的一个事务编号，Zxid是一个64位的数字，其中低32位是一个简单的单调递增计数器，针对客户端每一个事务请求，计数器加1；而高32位则代表Leader周期年代的编号
* Leader周期(epoch)：可以理解为当前集群所处的年代或者周期，每当有一个新的Leader选举出现时，就会从这个Leader服务器上取出其本地日志中最大事务的Zxid，并从中读取epoch值，然后加1，以此作为新得周期id。高32位代表了每代Leader的唯一性，低32位则代表了每代Leader中事务的唯一性
* zab节点的三种状态
  * following：服从Leader的命令
  * leading：负责协调事务
  * election/looking：选举状态

# 集群、分布式、SOA、微服务的概念及区别

* 集群：同一个业务，部署在多个服务器上。通过提高单位时间内执行的任务数来提升效率。集群是解决高可用的

* 分布式：一个业务分拆多个子业务，部署在不同的服务器上。以缩短单个任务的执行时间来提升效率的。分布式是解决高性能、高并发的

  > 例如：
  >
  > ​	如果一个任务由10个子任务组成，每个子任务单独执行需1小时，则在一台服务器上执行改任务需10小时。
  >
  > ​	采用分布式方案，提供10台服务器，每台服务器只负责处理一个子任务，不考虑子任务间的依赖关系，执行完这个任务只需一个小时。(这种工作模式的一个典型代表就是Hadoop的Map/Reduce分布式计算模型）
  >
  > ​	而采用集群方案，同样提供10台服务器，每台服务器都能独立处理这个任务。假设有10个任务同时到达，10个服务器将同时工作，10小后，10个任务同时完成，这样，整身来看，还是平均1小时完成一个任务！（注意这里的任务和子任务的区别）

* [SOA](https://blog.csdn.net/zpoison/article/details/80729052)：Service Oriented Ambiguity 即面向服务架构， 简称SOA。它是一个组件模型，它**将应用程序的不同功能单元（称为服务）进行拆分，并通过这些服务之间定义良好的接口和协议联系起来**。接口是采用中立的方式进行定义的，它应该**独立于实现服务的硬件平台、操作系统和编程语言**。这使得构建在各种各样的系统中的服务可以以一种**统一和通用**的方式进行交互

* [微服务](https://www.zhihu.com/question/65502802)：一种软件开发技术- SOA架构样式的一种变体，**将应用程序构造为一组松散耦合的服务**。在微服务体系结构中，服务是细粒度的，协议是轻量级的。微服务（或微服务架构）是一种云原生架构方法，其中**单个应用程序由许多松散耦合且可独立部署的较小组件或服务组成**。这些服务通常

  * 有自己的堆栈，包括数据库和数据模型

  * 通过REST API，事件流和消息代理的组合相互通信

  * 和它们是按业务能力组织的，分隔服务的线通常称为有界上下文

  * 尽管有关微服务的许多讨论都围绕体系结构定义和特征展开，但它们的价值可以通过相当简单的业务和组织收益更普遍地理解：
    * 可以更轻松地更新代码
    * 团队可以为不同的组件使用不同的堆栈
    * 组件可以彼此独立地进行缩放，从而减少了因必须缩放整个应用程序而产生的浪费和成本，因为单个功能可能面临过多的负载

  <img src="imgs/distribution/cluster_dis_soa_microservice.jpg" alt="cluster_dis_soa_microservice" style="zoom:80%;" />

# 分布式系统设计目标

* 可扩展性：通过对服务、存储的扩展，来提高系统的处理能力，通过对多台服务器协同工作，来完成单台服务器无法处理的任务，尤其是高并发或者大数据量的任务
* 高可用：单点不影响整体，单点故障指系统某个组件一旦失效，会让整个系统无法工作
* 无状态：无状态的服务才能满足部分宕机不影响全部，可以随时进行扩展的需求
* 可管理：便于运维，出问题能不能及时发现定位
* 高可靠：同样的请求返回同样的数据；更新能够持久化；数据不会丢失

# [分布式事务的解决方案](https://strikefreedom.top/distributed-transaction-theorems-uncovering)

* 基于**XA协议**：两阶段提交(2PC)和三阶段提交(3PC)，**需要数据库层面支持**
* 基于事务补偿机制：TCC，**基于业务层面实现**
* 本地消息表(用的少)：基于本地数据库+mq，维护本地状态(进行中)，通过mq调用服务，完成后响应一条消息回调，将状态改成完成。需要配合定时任务扫表、重新发送消息条用服务，需要保证幂等
* 基于事务消息：mq

## 两阶段提交

* 第一阶段(prepare)：每个参与者执行本地事务但不提交，进入ready状态，并通知协调者(coordinator)已经准备就绪
* 第二阶段(commit)：当协调者确认每个参与者都ready后，通知参与者进行commit操作；如果有参与者fail，则发送rollback命令，各参与者做回滚

<img src="imgs/distribution/2pc.png" alt="2pc" style="zoom:80%;" />

* 存在问题
  * 单点故障：一旦事务管理器(协调者)出现故障，整个系统不可用，参与者都会阻塞在未提交事务中，参与事务的数据会被锁住
  * 数据不一致：在阶段二，如果事务管理器只发送了部分commit信息，此时网络发生异常，那么只有部分参与者接受到commit消息，导致只有部分参与者提交了事务，使系统数据不一致
  * 响应时间较长：参与者和协调者资源都被锁住，提交或者回滚之后才能释放
  * 不确定性：当事务管理器发送commit之后，并且此时只有一个参与者收到了commit，那么当该参与者与事务管理器同时宕机之后，重新选举的事务管理器无法确定该条信息是否提交成功

## 三阶段提交

* 主要是针对两阶段的优化，解决2PC单点故障问题，但是性能问题和不一致性问题仍然没有根本解决
* 引入超时机制解决参与者阻塞问题，超时后本地提交，2PC只有协调者有超时机制
* 第一阶段(CanCommit)：协调者询问参与者，是否能够完成此次事务
  * 如果都返回yes，则进入第二阶段
  * 有一个返回no或者等待响应超时，则中断事务，并向所有参与者发送abort请求

* 第二阶段(PreCommit)：此时协调者会向所有参与者发送PreCommit请求，参与者收到后开始执行事务操作。参与者执行完事务后(处于ready状态)，就会向协调者反馈ack表示已经准备好提交，并等待协调者的下一指令
* 第三阶段(DoCommit)：在阶段二中
  * 如果所有参与者都返回ack，那么协调者会从预提交状态转变为提交状态，然后向所有参与者发送doCommit请求，参与者收到该请求后就会各自执行事务提交操作，并向协调者节点反馈ack消息。协调者收到所有参与者的ack消息后完成事务
  * 相反，如果有一个参与者未完成PreCommit的ack反馈或者反馈超时，那么协调者都会向所有参与者发送abort请求，从而中断事务
  * 参与者一直没有收到协调者的消息，等待超时之后会直接提交事务，因为CanCommit阶段探测通过，PreCommit完成的概率很大

<img src="imgs/distribution/3pc.png" alt="3pc" style="zoom: 67%;" />

## TCC 补偿事务

* 在对事务有要求，且不方便解耦的情况下，TCC 补偿式事务是个较好的选择
* TCC是业务层面的，不需要数据库支持

* TCC 把调用每个服务都分成 2 个阶段、 3 个操作：
  * 阶段一、**Try 操作**：对业务资源做检测、资源预留，比如对库存的检查、预扣
  * 阶段二、**Confirm 操作**：提交确认 Try 操作的资源预留。比如把库存预扣更新为扣除
  * 阶段二、**Cancel 操作**：Try 操作失败后，释放其预扣的资源。比如把库存预扣的加回去
* 事务处理过程：
  * TM首先发起所有的分支事务的try操作，任何一个分支事务try操作执行失败，TM将会发起所有分支事务的cancel操作进行回滚，若try操作全部成功，TM将会发起所有分支事务的confirm操作，其中confirm/cancel操作若执行失败，TM将会重试

* TCC模型对事务的侵入性比较强，改造难度大，要求每个服务都实现上面 3 个操作的接口，服务接入 TCC 事务前一次调用就完成的操作，现在需要分 2 阶段、三次操作来完成
* TCC中会条件事务日志，如果confirm或者cancel阶段出错，则会进行重试，所以这两个阶段需要支持幂等；如果重试失败，则需要人工介入进行恢复和处理等

# 如何理解RPC

* 远程过程调用：rpc要求在调用方中放置被调用方的接口。调用方只要调用了这些接口，就相当于调用了被调用方的实际方法，十分易用。于是，调用方可以像调用本地方法一样调用远程的方法，而不用封装参数名和参数值等操作
* rpc主要包含：
  * 动态代理，封装调用细节
  * 序列化和反序列化，数据传输与接收
  * 通信，可以选择七层的http，也可以选择四层的tcp/udp
  * 异常处理等
* 首先，调用方调用的是接口，必须得为接口构造一个假的实现。显然要使用动态代理，这样，调用方的调用就会被动态代理接收到
* 第二，动态代理接收到调用后，应该想办法调用远程的实际实现。这包括下面几步：
  * **识别具体要调用的远程方法的ip、端口**
  * 将调用方法的入参进行序列化
  * 通过通信将请求发送到远程的方法中
* 这样，远程的服务就接受到了调用方的请求。它应该：
  * 反序列化各个参数调用
  * 定位到实际要调用的方法，然后输入参数，执行方法
  * 按照调用的路径返回调用的结果
  
  <img src="imgs/distribution/rpc.png" alt="rpc" style="zoom:80%;" />

## 服务注册与发现

* 在分布式系统中，不同的服务之前该如何通信？
  * 传统的方式可以通过http请求调用、保存服务端的服务列表等
  * 这样做需要开发者主动感知到服务端暴露的信息，系统之间严重耦合

* RPC分布式系统中，服务端节点上线后自行向注册中心注册服务列表，节点下线时要从注册中心将节点元数据信息移除
* 节点从注册中心下线方式
  * 节点主动通知
    * 当节点需要下线时，想注册中心发送下线请求，让注册中心移除自己的元数据信息。这种方式只能用于服务主动下线情况，不能应对被动下线，如宕机、网络故障等
  * 主动通知+心跳检测
    * 除了主动通知注册中心下线外，还需要增加节点与注册中心的心跳检测功能，这个过程也叫做探活，心跳检测可以由节点或者注册中心负责
    * 优点：可以解耦客户端和服务端之间错综复杂的关系，并且能够实现对服务的动态管理

## 通信协议和序列化

* 客户端向服务端发起调用之前，需要考虑采用何种方式将调用信息进行编码，并传输到服务端
* RPC框架可以基于不同的协议实现
  * 大部分主流的RPC框架会选择TCP、HTTP
  * gRPC框架使用的是HTTP2
  * 也可以选择UDP协议

* 客户端和服务端在通信过程中需要传输那些数据？这些数据又该如何编解码？
  * 如果采用TCP协议，需要将调用的接口、方法、请求参数、调用属性等信息序列化成二进制字节流传递给服务提供方。服务端接收到数据后再把二进制字节流反序列化得到调用信息，然后利用反射原理调用对应方法，最后将返回结果、返回码、异常信息等返回给客户端
  * 常用的序列化算法：FastJson、Kryo、Hessian、Protobuf等

## RPC调用方式

<img src="imgs/distribution/rpc_call.png" alt="rpc_call" style="zoom:80%;" />

* 同步调用

  * 客户端调用远程服务后，会一直阻塞直到远程调用返回结果，或者调用返回异常

  <img src="imgs/distribution/rpc_call_sync.png" alt="rpc_call_sync" style="zoom:80%;" />

* Future异步调用

  * 客户端调用远程服务后，不会原地阻塞等待，而是立即返回一个Future对象，客户端可以决定何时到Future对象中获取调用结果

* Callback回调调用

  * 客户端调用远程服务时，将Callback对象传递给RPC框架，然后直接返回处理其他逻辑，但获取到服务端调用结果或者异常信息后，再调用用户注册的Callback回调通知用户

  <img src="imgs/distribution/rpc_call_callback.png" alt="rpc_call_callback" style="zoom:80%;" />

* Oneway单向调用

  * 客户端调用远程服务后，立即返回，且无需返回远程调用结果

  <img src="imgs/distribution/rpc_call_oneway.png" alt="rpc_call_oneway" style="zoom:80%;" />

## 线程模型

* 如果业务逻辑比较复杂，执行耗时比较长，包含读写数据库等操作，则应该将业务逻辑从IO线程中玻璃，分发到业务线程池中进行处理，以免阻塞IO线程

<img src="imgs/distribution/rpc_thread_model.png" alt="rpc_thread_model" style="zoom: 67%;" />

## 负载均衡

* 客户端在发起调用之前，需要感知有多少服务节点可用，然后从中选取一个进行调用
* 客户端需要拿到服务端节点的状态信息，并根据不同的策略实现负载均衡算法

## 动态代理

* RPC框架怎么做到像调用本地接口一样调用远程服务？
  * 必须依赖动态代理来实现，通过创建一个代理对象，在代理对象中完成数据报文编码，然后发起调用发送数据给服务提供方，以此屏蔽RPC框架的调用细节
  * 代理类的生成速度、生成的字节码大小
  * JDK动态代理、Cglib、Javassist、ASM、Byte Buddy
* JDK动态代理
  * 在运行时可以动态创建代理类
  * JDK动态代理的功能比较局限，代理对象必须实现一个接口，否则抛出异常
  * JDK动态代理通过反射调用的形式代理类中的方法

* Cglib动态代理
  * 基于ASM字节码生成框架实现
  * 通过字节码技术生成的代理类，所以代理类的类型是不受限制的
  * 采用了FastClass机制，为代理类和被代理类各自创建一个Class，这个Class会为代理类和被代理类的方法分配index索引，FastClass就可以通过index直接定位要调用的方法，并直接调用
